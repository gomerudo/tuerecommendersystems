{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code based on: https://github.com/sorenbouma/keras-oneshot/blob/master/load_data.py\n",
    "\n",
    "This script preprocesses the omniglot dataset (training and test set, stored as PNG files in subfolders) into a numpy array of shape (n_classes, n_examples, n_drawings, width, height), as well as a dictionary that can be referenced to find out which alphabet a particular (character) class belongs to. This data is then stored as pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading alphabet: Gujarati\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gomerudo/workspace/python_virtual/rs/lib/python3.6/site-packages/ipykernel_launcher.py:31: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading alphabet: Korean\n",
      "loading alphabet: Arcadian\n",
      "loading alphabet: Malay_(Jawi_-_Arabic)\n",
      "loading alphabet: Grantha\n",
      "loading alphabet: Blackfoot_(Canadian_Aboriginal_Syllabics)\n",
      "loading alphabet: Balinese\n",
      "loading alphabet: Futurama\n",
      "loading alphabet: N_Ko\n",
      "loading alphabet: Burmese_(Myanmar)\n",
      "loading alphabet: Anglo-Saxon_Futhorc\n",
      "loading alphabet: Mkhedruli_(Georgian)\n",
      "loading alphabet: Latin\n",
      "loading alphabet: Braille\n",
      "loading alphabet: Sanskrit\n",
      "loading alphabet: Japanese_(hiragana)\n",
      "loading alphabet: Tagalog\n",
      "loading alphabet: Greek\n",
      "loading alphabet: Ojibwe_(Canadian_Aboriginal_Syllabics)\n",
      "loading alphabet: Japanese_(katakana)\n",
      "loading alphabet: Early_Aramaic\n",
      "loading alphabet: Hebrew\n",
      "loading alphabet: Tifinagh\n",
      "loading alphabet: Asomtavruli_(Georgian)\n",
      "loading alphabet: Armenian\n",
      "loading alphabet: Syriac_(Estrangelo)\n",
      "loading alphabet: Alphabet_of_the_Magi\n",
      "loading alphabet: Cyrillic\n",
      "loading alphabet: Bengali\n",
      "loading alphabet: Inuktitut_(Canadian_Aboriginal_Syllabics)\n",
      "loading alphabet: Oriya\n",
      "loading alphabet: ULOG\n",
      "loading alphabet: Tengwar\n",
      "loading alphabet: Malayalam\n",
      "loading alphabet: Atlantean\n",
      "loading alphabet: Keble\n",
      "loading alphabet: Manipuri\n",
      "loading alphabet: Gurmukhi\n",
      "loading alphabet: Tibetan\n",
      "loading alphabet: Aurek-Besh\n",
      "loading alphabet: Ge_ez\n",
      "loading alphabet: Angelic\n",
      "loading alphabet: Old_Church_Slavonic_(Cyrillic)\n",
      "loading alphabet: Kannada\n",
      "loading alphabet: Avesta\n",
      "loading alphabet: Mongolian\n",
      "loading alphabet: Syriac_(Serto)\n",
      "loading alphabet: Atemayar_Qelisayer\n",
      "loading alphabet: Sylheti\n",
      "loading alphabet: Glagolitic\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.misc import imread\n",
    "import pickle\n",
    "import os\n",
    "\"\"\"Script to preprocess the omniglot dataset and pickle it into an array that's easy to index by character type\"\"\"\n",
    "\n",
    "data_path = os.path.join(\"data\", \"omniglot\")\n",
    "train_path = os.path.join(data_path, \"omniglot_train\")\n",
    "test_path = os.path.join(data_path,\"omniglot_test\")\n",
    "\n",
    "lang_dict = {}\n",
    "\n",
    "def loadimgs(path, n=0):\n",
    "    X=[]\n",
    "    y = []\n",
    "    cat_dict = {}\n",
    "    lang_dict = {}\n",
    "    curr_y = n\n",
    "    # we load every alphabet separately so we can isolate them later\n",
    "    for alphabet in os.listdir(path):\n",
    "        print(\"loading alphabet: \" + alphabet)\n",
    "        lang_dict[alphabet] = [curr_y,None]\n",
    "        alphabet_path = os.path.join(path,alphabet)\n",
    "        # every letter/category has its own column in the array, so load separately\n",
    "        for letter in os.listdir(alphabet_path):\n",
    "            cat_dict[curr_y] = (alphabet, letter)\n",
    "            category_images=[]\n",
    "            letter_path = os.path.join(alphabet_path, letter)\n",
    "            for filename in os.listdir(letter_path):\n",
    "                image_path = os.path.join(letter_path, filename)\n",
    "                image = imread(image_path)\n",
    "                category_images.append(image)\n",
    "                y.append(curr_y)\n",
    "            try:\n",
    "                X.append(np.stack(category_images))\n",
    "            #edge case  - last one\n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "                print(\"error - category_images:\", category_images)\n",
    "            curr_y += 1\n",
    "            lang_dict[alphabet][1] = curr_y - 1\n",
    "    y = np.vstack(y)\n",
    "    X = np.stack(X)\n",
    "    return X, y, lang_dict\n",
    "\n",
    "X, y, c = loadimgs(train_path)\n",
    "with open(os.path.join(data_path, \"omniglot_train.p\"), \"wb\") as f:\n",
    "    pickle.dump((X, c), f)\n",
    "\n",
    "X, y, c = loadimgs(test_path)\n",
    "with open(os.path.join(data_path, \"omniglot_test.p\"), \"wb\") as f:\n",
    "    pickle.dump((X, c), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
