{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code based on: https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data (MNIST)\n",
    "We obtain the MNIST data set directly through Keras, given as fully labelled training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each example is an image of 28x28 pixels, given as integer grayscale values from 0 to 255. Each example has a label, an integer 0 to 9. The training set contains 60,000 examples, the test set 10,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use matplotlib to quickly visualise some of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAAECCAYAAAAYUakXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nldBISwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2645db496a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_id = 0  # pick any integer from 0 to 59999 to visualize a training example\n",
    "example = x_train[example_id]\n",
    "label = y_train[example_id]\n",
    "print(\"Class label:\", label)\n",
    "plt.matshow(example, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST digits have 28\\*28=784 dimensions/pixels, and belong to one of 10 possible classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_dims = 784  # MNIST digits have 28*28=784 dimensions/pixels\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are going to build a multi-layer perceptron, which uses Dense layers. These layers expect the data to be given as vectors, not matrices.\n",
    "* The pixel values are given by integer values from 0 to 255, we normalise this to obtain float values from 0 to 1.\n",
    "* Labels are given as values 0 to 9, but here we need so-called \"one-hot\" encodings, e.g. 3 becomes [0,0,0,1,0,0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000, n_dims)\n",
    "x_test = x_test.reshape(10000, n_dims)\n",
    "\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "y_train = to_categorical(y_train, n_classes)\n",
    "y_test = to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how the shapes of the datasets have changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "example one-hot encoding: [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(\"example one-hot encoding:\", y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model architecture & settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build an MLP with two hidden layers, with the given number of hidden units. We also include Dropout for each of the layers, with the given dropout rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intermediate_dim1 = 256\n",
    "intermediate_dim2 = 128\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initialise a Keras Sequential model\n",
    "* Add two hidden (Dense) layers with ReLU activations and dropout, then a (Dense) Softmax layer with 10 classes (to obtain classification predictions summing up to 1). The first layer must explicitly receive the shape of the input, following layers can do automatice shape inference.\n",
    "* Optional: print a summary of the model\n",
    "* Compile the model with the following settings:\n",
    "    * use stochastic gradient descent with the \"adadelta\" optimizer to train the model\n",
    "    * MNIST is a multi-class classification problem, use categorical cross entropy loss function\n",
    "    * output accuracy (% of correctly classified instances) when evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 235,146\n",
      "Trainable params: 235,146\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(intermediate_dim1, activation=\"relu\", input_shape=(n_dims,)))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(intermediate_dim2, activation=\"relu\"))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(n_classes, activation=\"softmax\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"adadelta\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "Train the model (using stochastic gradient descent) with given batch size, for given number of epochs. We split of 1/12-th of the data (5,000 of the 60,000 samples) as validation data, such that we can use the validation accuracy for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.1746 - acc: 0.9488 - val_loss: 0.0975 - val_acc: 0.9744\n",
      "Epoch 2/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.1701 - acc: 0.9496 - val_loss: 0.0961 - val_acc: 0.9740\n",
      "Epoch 3/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.1672 - acc: 0.9515 - val_loss: 0.0948 - val_acc: 0.9744\n",
      "Epoch 4/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.1629 - acc: 0.9512 - val_loss: 0.0930 - val_acc: 0.9760\n",
      "Epoch 5/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.1560 - acc: 0.9541 - val_loss: 0.0906 - val_acc: 0.9756\n",
      "Epoch 6/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.1539 - acc: 0.9545 - val_loss: 0.0897 - val_acc: 0.9758\n",
      "Epoch 7/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.1512 - acc: 0.9551 - val_loss: 0.0884 - val_acc: 0.9760\n",
      "Epoch 8/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.1462 - acc: 0.9574 - val_loss: 0.0860 - val_acc: 0.9766\n",
      "Epoch 9/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.1439 - acc: 0.9574 - val_loss: 0.0855 - val_acc: 0.9760\n",
      "Epoch 10/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.1412 - acc: 0.9584 - val_loss: 0.0843 - val_acc: 0.9766\n",
      "Epoch 11/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.1383 - acc: 0.9593 - val_loss: 0.0834 - val_acc: 0.9766\n",
      "Epoch 12/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.1342 - acc: 0.9607 - val_loss: 0.0818 - val_acc: 0.9780\n",
      "Epoch 13/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.1319 - acc: 0.9612 - val_loss: 0.0811 - val_acc: 0.9778\n",
      "Epoch 14/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.1296 - acc: 0.9618 - val_loss: 0.0797 - val_acc: 0.9778\n",
      "Epoch 15/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.1270 - acc: 0.9627 - val_loss: 0.0792 - val_acc: 0.9786\n",
      "Epoch 16/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.1268 - acc: 0.9630 - val_loss: 0.0783 - val_acc: 0.9792\n",
      "Epoch 17/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.1246 - acc: 0.9634 - val_loss: 0.0777 - val_acc: 0.9798\n",
      "Epoch 18/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.1205 - acc: 0.9651 - val_loss: 0.0762 - val_acc: 0.9802\n",
      "Epoch 19/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.1185 - acc: 0.9649 - val_loss: 0.0758 - val_acc: 0.9798\n",
      "Epoch 20/20\n",
      "55000/55000 [==============================] - 2s - loss: 0.1176 - acc: 0.9656 - val_loss: 0.0747 - val_acc: 0.9808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2646eac9320>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 20\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=1/12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the model using the test set, obtaining the test loss and accuracy (% examples correctly classified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.135846870592\n",
      "Test accuracy: 0.959\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
