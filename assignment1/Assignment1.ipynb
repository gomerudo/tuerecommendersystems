{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "**Group 22**\n",
    "\n",
    "A. Siganos ( 1283871 )     \n",
    "J. Gómez Robles ( 1286552 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(13) #TODO Check if this is used for sgd\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Reshape, Lambda\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.preprocessing import sequence\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors as nn\n",
    "from matplotlib import pylab\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT Modify the lines in this cell\n",
    "path = 'alice.txt'\n",
    "corpus = open(path).readlines()[0:700]\n",
    "\n",
    "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
    "\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"'\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "corpus = tokenizer.texts_to_sequences(corpus)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "V = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Is this something they need to change?\n",
    "dim = 100\n",
    "window_size = 2 #use this window size for Skipgram, CBOW, and the model with the additional hidden layer\n",
    "window_size_corpus = 4 #use this window size for the co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "### Co-occurrence Matrix\n",
    "Use the provided code to load the \"Alice in Wonderland\" text document. \n",
    "1. Implement the word-word co-occurrence matrix for “Alice in Wonderland”\n",
    "2. Normalize the words such that every value lies within a range of 0 and 1\n",
    "3. Compute the cosine distance between the given words:\n",
    "    - Alice \n",
    "    - Dinah\n",
    "    - Rabbit\n",
    "4. List the 5 closest words to 'Alice'. Discuss the results.\n",
    "5. Discuss what the main drawbacks are of a term-term co-occurence matrix solutions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "############################## 0. Util functions ###############################\n",
    "################################################################################\n",
    "\n",
    "# Run this block first\n",
    "def getKeyFromValue(dictionary, target):\n",
    "    return list(dictionary.keys())[list(dictionary.values()).index(target)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.93421053, ..., 0.01315789, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.93421053, 0.        , ..., 0.01315789, 0.01315789,\n",
       "        0.01315789],\n",
       "       ...,\n",
       "       [0.        , 0.01315789, 0.01315789, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.01315789, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.01315789, ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################\n",
    "################ 1a. Computing word-word co-ocurrence matrix ###################\n",
    "################################################################################\n",
    "\n",
    "# Not changing V in the cell they do not want us to modify, but one never uses a \n",
    "# size variable to handle an iteration issue.\n",
    "#V = len(tokenizer.word_index)\n",
    "\n",
    "# Create my co-ocurrence matrix, initially 0 (VxV size)\n",
    "# How to access this matrix:\n",
    "#   Each column is an (index - 1) (column 0 is word in index 1; column 1 is word in index 2; ...)\n",
    "wcoMatrix = np.zeros([V, V]) \n",
    "\n",
    "# Using my window_size_corpus to define my context scope\n",
    "scope = window_size_corpus\n",
    "\n",
    "# TODO: Do not make that many iterations\n",
    "# Greedy approach first (to be able to compare the optimization)\n",
    "\n",
    "# For each line in the corpus. Note that they preserve the order, even when they are indexes now.\n",
    "for s in corpus:\n",
    "    # For each word\n",
    "    for current_index in range(0, len(s)):\n",
    "        current_value = s[current_index] # Get the 'word'\n",
    "        # From left to right\n",
    "        for neighbor_index in range(current_index - scope, current_index + scope + 1):\n",
    "            # Never out of boundaries and never the same index\n",
    "            if ( neighbor_index >= 0 and neighbor_index < len(s) ) and ( neighbor_index != current_index ):\n",
    "                # Get my neighbor 'word'\n",
    "                neighbor = s[neighbor_index] \n",
    "                # Never myself and myself (keeping diagonal to 0)\n",
    "                if current_value != neighbor:\n",
    "                    # Update the ocurrence\n",
    "                    #wcoMatrix[current_value - 1, neighbor - 1] += 1\n",
    "                    wcoMatrix[current_value, neighbor] += 1\n",
    "                    \n",
    "# Finally normalizing\n",
    "wcoMatrix = wcoMatrix / wcoMatrix.max()\n",
    "wcoMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity(alice, dinah)=[[0.39360011]]\n",
      "cosine_similarity(alice, rabbit)=[[0.47890931]]\n",
      "cosine_similarity(dinah, alice)=[[0.39360011]]\n",
      "cosine_similarity(dinah, rabbit)=[[0.29862324]]\n",
      "cosine_similarity(rabbit, alice)=[[0.47890931]]\n",
      "cosine_similarity(rabbit, dinah)=[[0.29862324]]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "############ 1b. Find cosine similarity to Alice, Dinah and Rabbit #############\n",
    "################################################################################\n",
    "\n",
    "words_to_compare = [\"Alice\", \"Dinah\", \"Rabbit\"]\n",
    "\n",
    "# Get my index\n",
    "full_index = tokenizer.word_index\n",
    "\n",
    "# Iterate on all possible combinations\n",
    "for w1 in words_to_compare:\n",
    "    w1 = w1.lower() \n",
    "    x = full_index[w1]\n",
    "    for w2 in words_to_compare:\n",
    "        w2 = w2.lower()\n",
    "        if w1 != w2:\n",
    "            y = full_index[w2]\n",
    "            X = wcoMatrix[x, :].reshape((1, V))\n",
    "            Y = wcoMatrix[y, :].reshape((1, V))\n",
    "            print(\"cosine_similarity(%s, %s)=%s\" % (w1, w2, cosine_similarity(X, Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors (co-ocurrence matrix based) to alice (index 11):\n",
      "that [index 13] with distance: 0.5692975698101006\n",
      "herself [index 41] with distance: 0.5700573426956416\n",
      "for [index 18] with distance: 0.5811858638252018\n",
      "on [index 21] with distance: 0.5837124477057559\n",
      "be [index 20] with distance: 0.5890270864631393\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "##################### 1c. Find the closest words to Alice #######################\n",
    "################################################################################\n",
    "\n",
    "# It was not importing\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Define our variables\n",
    "targetWord = \"Alice\".lower()\n",
    "X = wcoMatrix\n",
    "N_NEIGHBORS = 6\n",
    "full_index = tokenizer.word_index\n",
    "\n",
    "# Perfrom the NearestNeibhbors algorithm\n",
    "# TODO: Tune parameters\n",
    "nbrs = NearestNeighbors(n_neighbors = N_NEIGHBORS).fit(X)\n",
    "\n",
    "# Get the index corresponding to the target word\n",
    "aliceMatrixIndex = full_index[targetWord]\n",
    "\n",
    "# Get results from nearest neighbors\n",
    "distances, indices = nbrs.kneighbors()\n",
    "aliceIndices = indices[aliceMatrixIndex, :]\n",
    "aliceDistances = distances[aliceMatrixIndex, :]\n",
    "\n",
    "# Output result\n",
    "print(\"Nearest neighbors (co-ocurrence matrix based) to %s (index %s):\" % (targetWord, aliceMatrixIndex))\n",
    "for i in range(1, N_NEIGHBORS):\n",
    "    print( \"%s [index %s] with distance: %s\" % (getKeyFromValue(full_index, aliceIndices[i]), aliceIndices[i], aliceDistances[i]) )\n",
    "\n",
    "# Note: Not sure if the co-ocurrence matrix is correct or we should create a matrix with the cosine similarity\n",
    "#       between each pair of words (that is the real distance between words, isn't?)\n",
    "\n",
    "# # Create overall cosine similarity matrix\n",
    "# cosSimMatrix = np.zeros([V, V]) \n",
    "# for k1, v1 in enumerate(full_index):\n",
    "#     x = full_index[v1]\n",
    "#     for k2, v2 in enumerate(full_index):\n",
    "#         if v1 != v2:\n",
    "#             y = full_index[v2]\n",
    "#             X = wcoMatrix[x, :].reshape((1, V))\n",
    "#             Y = wcoMatrix[y, :].reshape((1, V))\n",
    "\n",
    "# X = cosSimMatrix\n",
    "# nbrs = NearestNeighbors(n_neighbors = N_NEIGHBORS + 1).fit(X)\n",
    "\n",
    "# # Get results from nearest neighbors\n",
    "# distances, indices = nbrs.kneighbors()\n",
    "# aliceIndices = indices[aliceMatrixIndex, :]\n",
    "# aliceDistances = distances[aliceMatrixIndex, :]\n",
    "\n",
    "# # Output result\n",
    "# print(\"\\nNearest neighbors (cosine similarity matrix based) to %s (index %s):\" % (targetWord, aliceMatrixIndex))\n",
    "# for i in range(1, N_NEIGHBORS):\n",
    "#     if aliceIndices[i] != 0:\n",
    "#         print( \"%s [index %s] with distance: %s\" % (getKeyFromValue(full_index, aliceIndices[i]), aliceIndices[i], aliceDistances[i]) )\n",
    "\n",
    "# TODO: Think about what we are computing. \n",
    "# With first approach, a neighbor would be a word with a similar vector in the co-ocurrence matrix, \n",
    "# not a word within the same context (window_size). Although words in the similar window_size should have a\n",
    "# similar vector, isn't?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion of the drawbacks**\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save your all the vector representations of your word embeddings in this way\n",
    "#Change when necessary the sizes of the vocabulary/embedding dimension\n",
    "\n",
    "f = open('vectors_co_occurrence.txt',\"w\")\n",
    "f.write(\" \".join([str(V-1),str(V-1)]))\n",
    "f.write(\"\\n\")\n",
    "\n",
    "#vectors = your word co-occurrence matrix\n",
    "vectors = wcoMatrix\n",
    "for word, i in tokenizer.word_index.items():    \n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid vector on line 0 (is this really the text format?)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-2766f8441a4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#reopen your file as follows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# TODO: Check! It is not reloading the file D:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mco_occurrence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./vectors_co_occurrence.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/workspace/python_virtual/rs/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1002\u001b[0m             \u001b[0mWord2VecKeyedVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/python_virtual/rs/lib/python3.6/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid vector on line %s (is this really the text format?)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mline_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m                 \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid vector on line 0 (is this really the text format?)"
     ]
    }
   ],
   "source": [
    "#reopen your file as follows\n",
    "# TODO: Check! It is not reloading the file D:\n",
    "co_occurrence = KeyedVectors.load_word2vec_format('./vectors_co_occurrence.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "### Word embeddings\n",
    "Build embeddings with a keras implementation where the embedding vector is of length 50, 150 and 300. Use the Alice in Wonderland text book for training.\n",
    "1. Using the CBOW model\n",
    "2. Using Skipgram model\n",
    "3. Add extra hidden dense layer to CBow and Skipgram implementations. Choose an activation function for that layer and justify your answer.\n",
    "4. Analyze the four different word embeddings\n",
    "    - Implement your own function to perform the analogy task with. Do not use existing libraries for this task such as Gensim. Your function should be able to answer whether an anaology as in the example given in the pdf-file is true.\n",
    "    - Compare the performance on the analogy task between the word embeddings that you have trained in 2.1, 2.2 and 2.3.  \n",
    "    - Visualize your results and interpret your results\n",
    "5. Use the word co-occurence matrix from Question 1. Compare the performance on the analogy task with the performance of your trained word embeddings.  \n",
    "6. Discuss:\n",
    "    - What are the main advantages of CBOW and Skipgram?\n",
    "    - What is the advantage of negative sampling?\n",
    "    - What are the main drawbacks of CBOW and Skipgram?\n",
    "7. Load pre-trained embeddings on large corpuses (see the pdf file). You only have to consider the word embeddings with an embedding size of 300\n",
    "    - Compare performance on the analogy task with your own trained embeddings from \"Alice in Wonderland\". You can limit yourself to the vocabulary of Alice in Wonderland. Visualize the pre-trained word embeddings and compare these with the results of your own trained word embeddings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for cbow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create CBOW model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for Skipgram\n",
    "\n",
    "# This function creates a list where each word in the text file (with non-empty content)\n",
    "# is encoded into two arrays:\n",
    "#   - x: an array of the size of all his windows_size neighbors with the value of the word(i.e, repeated values)\n",
    "#   - y: an array of the same size as x pointing to each neighbor. Finally encoded in a |V|-list as one-hot enc\n",
    "def generate_data_skipgram(corpus, window_size, V):\n",
    "    all_in = [] \n",
    "    all_out = []\n",
    "    for sentence in corpus: # For, sentence in corpus\n",
    "        L = len(sentence) # Limit for the iteration (per sentence)\n",
    "\n",
    "        # For each pair in the index\n",
    "        for index, word in enumerate(sentence):\n",
    "            # Create X and Y\n",
    "            in_words = [] # list of lists\n",
    "            labels = [] # list of integers\n",
    "\n",
    "            # Iterate over all the sentence, for each word in the line. I.e, for each word:\n",
    "            for i in range(index - window_size, index + window_size + 1):\n",
    "                # If between the limits and not myself\n",
    "                if i != index and (0 <= i < L):\n",
    "                    # This means: Associating the words in my neighbor (context) \n",
    "                    in_words.append([word]) # Current word (as a list) is associated to...\n",
    "                    labels.append(sentence[i]) # ... this word (as a simple integer).\n",
    "                    \n",
    "            # Once I have saved the context for the current word...\n",
    "            # ... if there is something in the arrays\n",
    "            if in_words != []:\n",
    "                # Save the information to the all_in/out arrays\n",
    "                all_in.append(np.array(in_words,dtype=np.int32)) # X in integer format\n",
    "                # Set Y to categorical values. If my output is ([6, 26]). Then I create\n",
    "                # a vector from 0 to V and mark 6 and 26 as 1\n",
    "                all_out.append(np_utils.to_categorical(labels, V))\n",
    "    return (all_in,all_out) # return the values\n",
    "\n",
    "# Prepare data for skipgram\n",
    "X, Y = generate_data_skipgram(corpus, window_size, V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save the preprocessed data of Skipgram\n",
    "# f = open('data_skipgram.txt' ,'w')\n",
    "\n",
    "# for input, outcome in zip(X,Y):\n",
    "#     input = np.concatenate(input)\n",
    "#     f.write(\" \".join(map(str, list(input))))\n",
    "#     f.write(\",\")\n",
    "#     outcome = np.concatenate(outcome)\n",
    "#     f.write(\" \".join(map(str,list(outcome))))\n",
    "#     f.write(\"\\n\")\n",
    "# f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load the preprocessed Skipgram data\n",
    "# def generate_data_skipgram_from_file():\n",
    "#     f = open('data_skipgram.txt' ,'r')\n",
    "#     for row in f:\n",
    "#         inputs,outputs = row.split(\",\")\n",
    "#         inputs = np.fromstring(inputs, dtype=int, sep=' ')\n",
    "#         inputs = np.asarray(np.split(inputs, len(inputs)))\n",
    "#         outputs = np.fromstring(outputs, dtype=float, sep=' ')\n",
    "#         outputs = np.asarray(np.split(outputs, len(inputs)))\n",
    "#         yield (inputs,outputs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Skipgram model\n",
    "\n",
    "# The Sequential model is a linear stack of layers.\n",
    "\n",
    "skipgram = Sequential()\n",
    "skipgram.add(Embedding(input_dim = V, output_dim = dim, embeddings_initializer = 'glorot_uniform', input_length = 1))\n",
    "skipgram.add(Reshape((dim, )))\n",
    "skipgram.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function for Skipgram\n",
    "skipgram.compile(loss='categorical_crossentropy', optimizer='adadelta')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected reshape_8 to have shape (100,) but got array with shape (1183,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-778d4956c59b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mskipgram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/python_virtual/rs/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1065\u001b[0m         return self.model.train_on_batch(x, y,\n\u001b[1;32m   1066\u001b[0m                                          \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m                                          class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m     def test_on_batch(self, x, y,\n",
      "\u001b[0;32m~/workspace/python_virtual/rs/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1882\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1883\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1884\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1885\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1886\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/python_virtual/rs/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1485\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1488\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1489\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m~/workspace/python_virtual/rs/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    121\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected reshape_8 to have shape (100,) but got array with shape (1183,)"
     ]
    }
   ],
   "source": [
    "#train Skipgram model\n",
    "for ite in range(10):\n",
    "    loss = 0.\n",
    "    for x, y in zip(X, Y):\n",
    "        loss += skipgram.train_on_batch(x, y)\n",
    "\n",
    "    print(ite, loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create CBOW model with additional dense layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function for CBOW + dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model for CBOW + dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Skipgram with additional dense layer\n",
    "\n",
    "skipgram = Sequential()\n",
    "skipgram.add(Embedding(input_dim = V, output_dim = dim, embeddings_initializer = 'glorot_uniform', input_length = 1))\n",
    "skipgram.add(Reshape((dim, )))\n",
    "skipgram.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function for Skipgram + dense\n",
    "skipgram.compile(loss='categorical_crossentropy', optimizer='adadelta')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 41242.59084939957\n",
      "1 38998.47589588165\n",
      "2 39104.93803155422\n",
      "3 39152.549574136734\n",
      "4 39207.58228170872\n",
      "5 39277.39926147461\n",
      "6 39361.115528941154\n",
      "7 39453.87337732315\n",
      "8 39547.44167852402\n",
      "9 39634.98483276367\n"
     ]
    }
   ],
   "source": [
    "#train model for Skipgram + dense\n",
    "#train Skipgram model\n",
    "for ite in range(10):\n",
    "    loss = 0.\n",
    "    for x, y in zip(X, Y):\n",
    "        loss += skipgram.train_on_batch(x, y)\n",
    "\n",
    "    print(ite, loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement your own analogy function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization results trained word embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation results of the visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results of the trained word embeddings with the word-word co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of the advantages of CBOW and Skipgram, the advantages of negative sampling and drawbacks of CBOW and Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pretrained word embeddings of word2vec\n",
    "\n",
    "path_word2vec = \"your path /GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pretraind word embeddings of Glove\n",
    "\n",
    "path = \"your path /glove.6B/glove.6B.300d_converted.txt\"\n",
    "\n",
    "#convert GloVe into word2vec format\n",
    "gensim.scripts.glove2word2vec.get_glove_info(path)\n",
    "gensim.scripts.glove2word2vec.glove2word2vec(path, \"glove_converted.txt\")\n",
    "\n",
    "glove = KeyedVectors.load_word2vec_format(path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison performance with your own trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
