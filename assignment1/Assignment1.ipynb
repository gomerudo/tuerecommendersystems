{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "**Group 22**\n",
    "\n",
    "A. Siganos ( 1283871 )     \n",
    "J. Gómez Robles ( 1286552 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(13) #TODO Check if this is used for sgd\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Reshape, Lambda\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.preprocessing import sequence\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors as nn\n",
    "from matplotlib import pylab\n",
    "from __future__ import division\n",
    "\n",
    "# To measure running times\n",
    "import time\n",
    "#from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Only for the word2vec parsing of last exercise (loading word2vec with gensim was part of the initial template)\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT Modify the lines in this cell\n",
    "path = 'alice.txt'\n",
    "corpus = open(path).readlines()[0:700]\n",
    "\n",
    "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
    "\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"'\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "corpus = tokenizer.texts_to_sequences(corpus)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "V = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Is this something they need to change?\n",
    "dim = 100\n",
    "window_size = 2 #use this window size for Skipgram, CBOW, and the model with the additional hidden layer\n",
    "window_size_corpus = 4 #use this window size for the co-occurrence matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "### Co-occurrence Matrix\n",
    "Use the provided code to load the \"Alice in Wonderland\" text document. \n",
    "1. Implement the word-word co-occurrence matrix for “Alice in Wonderland”\n",
    "2. Normalize the words such that every value lies within a range of 0 and 1\n",
    "3. Compute the cosine distance between the given words:\n",
    "    - Alice \n",
    "    - Dinah\n",
    "    - Rabbit\n",
    "4. List the 5 closest words to 'Alice'. Discuss the results.\n",
    "5. Discuss what the main drawbacks are of a term-term co-occurence matrix solutions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "############################## 0. Util functions ###############################\n",
    "################################################################################\n",
    "\n",
    "# Run this block first\n",
    "def getKeyFromValue(dictionary, target):\n",
    "    return list(dictionary.keys())[list(dictionary.values()).index(target)]\n",
    "\n",
    "plot_labels = []\n",
    "\n",
    "def update_annot(ind):\n",
    "    pos = sc.get_offsets()[ind[\"ind\"][0]]\n",
    "    annot.xy = pos\n",
    "    text = \"{}, {}\".format(\" \".join(list(map(str,ind[\"ind\"]))), \n",
    "                           \" \".join([plot_labels[n] for n in ind[\"ind\"]]))\n",
    "    annot.set_text(text)\n",
    "    annot.get_bbox_patch().set_facecolor(cmap(norm(c[ind[\"ind\"][0]])))\n",
    "    annot.get_bbox_patch().set_alpha(0.4)\n",
    "\n",
    "\n",
    "def hover(event):\n",
    "    vis = annot.get_visible()\n",
    "    if event.inaxes == ax:\n",
    "        cont, ind = sc.contains(event)\n",
    "        if cont:\n",
    "            update_annot(ind)\n",
    "            annot.set_visible(True)\n",
    "            fig.canvas.draw_idle()\n",
    "        else:\n",
    "            if vis:\n",
    "                annot.set_visible(False)\n",
    "                fig.canvas.draw_idle()\n",
    "\n",
    "\n",
    "# Visualization function\n",
    "def plot_embedding(x, labels, title):\n",
    "    fig,ax = plt.subplots()\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    plt.title(title, fontsize=20)\n",
    "    sc = plt.scatter(x[:,0],x[:,1], s=5)\n",
    "\n",
    "    annot = ax.annotate(\"\", xy=(0,0), xytext=(20,20),textcoords=\"offset points\",\n",
    "                        bbox=dict(boxstyle=\"round\", fc=\"w\"),\n",
    "                        arrowprops=dict(arrowstyle=\"->\"))\n",
    "    annot.set_visible(False)\n",
    "\n",
    "    fig.canvas.mpl_connect(\"motion_notify_event\", hover)\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sumary of the word-word co-ocurrence matrix\n",
      " - Shape (1183, 1183)\n",
      " - Min value 0.0\n",
      " - Max value 1.0\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "################# 1. Implement word-word co-ocurrence matrix ###################\n",
    "################################################################################\n",
    "\n",
    "# Create my co-ocurrence matrix, initially 0 (VxV size)\n",
    "# How to access this matrix:\n",
    "#   Each column is an (index - 1) (column 0 is word in index 1; column 1 is word in index 2; ...)\n",
    "wcoMatrix = np.zeros([V, V]) \n",
    "\n",
    "# Using my window_size_corpus to define my context scope\n",
    "scope = window_size_corpus\n",
    "\n",
    "# TODO: Do not make that many iterations\n",
    "# Greedy approach first (to be able to compare the optimization)\n",
    "\n",
    "# For each line in the corpus. Note that they preserve the order, even when they are indexes now.\n",
    "for s in corpus:\n",
    "    # For each word\n",
    "    for current_index in range(0, len(s)):\n",
    "        current_value = s[current_index] # Get the 'word'\n",
    "        # From left to right\n",
    "        for neighbor_index in range(current_index - scope, current_index + scope + 1):\n",
    "            # Never out of boundaries and never the same index\n",
    "            if ( neighbor_index >= 0 and neighbor_index < len(s) ) and ( neighbor_index != current_index ):\n",
    "                # Get my neighbor 'word'\n",
    "                neighbor = s[neighbor_index] \n",
    "                # Never myself and myself (keeping diagonal to 0)\n",
    "                if current_value != neighbor:\n",
    "                    # Update the ocurrence\n",
    "                    #wcoMatrix[current_value - 1, neighbor - 1] += 1\n",
    "                    wcoMatrix[current_value, neighbor] += 1\n",
    "                    \n",
    "################################################################################\n",
    "################################# 2. Normalize #################################\n",
    "################################################################################\n",
    "wcoMatrix = wcoMatrix / wcoMatrix.max()\n",
    "wcoMatrix\n",
    "\n",
    "print(\"Sumary of the word-word co-ocurrence matrix\")\n",
    "print(\" - Shape\", wcoMatrix.shape)\n",
    "print(\" - Min value\", wcoMatrix.min())\n",
    "print(\" - Max value\", wcoMatrix.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity(alice, dinah)=[[0.39360011]]\n",
      "cosine_similarity(alice, rabbit)=[[0.47890931]]\n",
      "cosine_similarity(dinah, alice)=[[0.39360011]]\n",
      "cosine_similarity(dinah, rabbit)=[[0.29862324]]\n",
      "cosine_similarity(rabbit, alice)=[[0.47890931]]\n",
      "cosine_similarity(rabbit, dinah)=[[0.29862324]]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "########### 3. Compute cosine similarity to Alice, Dinah and Rabbit ############\n",
    "################################################################################\n",
    "\n",
    "words_to_compare = [\"Alice\", \"Dinah\", \"Rabbit\"]\n",
    "\n",
    "# Get my index\n",
    "full_index = tokenizer.word_index\n",
    "\n",
    "# Iterate on all possible combinations\n",
    "for w1 in words_to_compare:\n",
    "    w1 = w1.lower() \n",
    "    x = full_index[w1]\n",
    "    for w2 in words_to_compare:\n",
    "        w2 = w2.lower()\n",
    "        if w1 != w2:\n",
    "            y = full_index[w2]\n",
    "            X = wcoMatrix[x, :].reshape((1, V))\n",
    "            Y = wcoMatrix[y, :].reshape((1, V))\n",
    "            print(\"cosine_similarity(%s, %s)=%s\" % (w1, w2, cosine_similarity(X, Y)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors (co-ocurrence matrix based) to alice (index 11):\n",
      "her [index 14] with distance: 0.5616433150108668\n",
      "that [index 13] with distance: 0.5692975698101006\n",
      "herself [index 41] with distance: 0.5700573426956416\n",
      "for [index 18] with distance: 0.5811858638252018\n",
      "on [index 21] with distance: 0.5837124477057559\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "################## 4a. List the 5 closest words to 'Alice'. ####################\n",
    "################################################################################\n",
    "\n",
    "################################\n",
    "### We use Nearest Neighbors ###\n",
    "################################\n",
    "\n",
    "# It was not importing\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Define our variables\n",
    "targetWord = \"Alice\".lower()\n",
    "X = wcoMatrix\n",
    "N_NEIGHBORS = 5\n",
    "full_index = tokenizer.word_index\n",
    "\n",
    "# Perfrom the NearestNeibhbors algorithm\n",
    "# TODO: Tune parameters\n",
    "nbrs = NearestNeighbors(n_neighbors = N_NEIGHBORS).fit(X)\n",
    "\n",
    "# Get the index corresponding to the target word\n",
    "aliceMatrixIndex = full_index[targetWord]\n",
    "\n",
    "# Get results from nearest neighbors\n",
    "distances, indices = nbrs.kneighbors()\n",
    "aliceIndices = indices[aliceMatrixIndex, :]\n",
    "aliceDistances = distances[aliceMatrixIndex, :]\n",
    "\n",
    "# Output result\n",
    "print(\"Nearest neighbors (co-ocurrence matrix based) to %s (index %s):\" % (targetWord, aliceMatrixIndex))\n",
    "for i in range(0, N_NEIGHBORS):\n",
    "    print( \"%s [index %s] with distance: %s\" % (getKeyFromValue(full_index, aliceIndices[i]), aliceIndices[i], aliceDistances[i]) )\n",
    "\n",
    "# TODO: Think about what we are computing. \n",
    "# With first approach, a neighbor would be a word with a similar vector in the co-ocurrence matrix, \n",
    "# not a word within the same context (window_size). Although words in the similar window_size should have a\n",
    "# similar vector, isn't?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4b. Discuss the results**\n",
    "\n",
    "We observe that the most similar words are possesives and prepositions. Although these results intuitively make some sense (Alice is the main character of the book and consequently, describing her behaviour is expected), no idiomatic relations can be achieved by means of the simple word-word co-ocurrence matrix.\n",
    "\n",
    "Furthermore, the minimal distance (cosine similarity normalized) found for alice is of 0.5616, which suggests a poor result - one would be interested on a similarity close to 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Discuss what the main drawbacks are of a term-term co-occurence matrix solutions? **\n",
    "\n",
    "TODO. Add some ideas!\n",
    "\n",
    "Jorge's ideas: sparse matrix VxV size (computation time increases).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save your all the vector representations of your word embeddings in this way\n",
    "#Change when necessary the sizes of the vocabulary/embedding dimension\n",
    "\n",
    "f = open('vectors_co_occurrence.txt',\"w\")\n",
    "f.write(\" \".join([str(V-1),str(V-1)]))\n",
    "f.write(\"\\n\")\n",
    "\n",
    "#vectors = your word co-occurrence matrix\n",
    "vectors = wcoMatrix\n",
    "for word, i in tokenizer.word_index.items():    \n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reopen your file as follows\n",
    "# TODO: Check! It is not reloading the file D:\n",
    "# co_occurrence = KeyedVectors.load_word2vec_format('./vectors_co_occurrence.txt', binary=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "### Word embeddings\n",
    "Build embeddings with a keras implementation where the embedding vector is of length 50, 150 and 300. Use the Alice in Wonderland text book for training.\n",
    "1. Using the CBOW model\n",
    "2. Using Skipgram model\n",
    "3. Add extra hidden dense layer to CBow and Skipgram implementations. Choose an activation function for that layer and justify your answer.\n",
    "4. Analyze the four different word embeddings\n",
    "    - Implement your own function to perform the analogy task with. Do not use existing libraries for this task such as Gensim. Your function should be able to answer whether an anaology as in the example given in the pdf-file is true.\n",
    "    - Compare the performance on the analogy task between the word embeddings that you have trained in 2.1, 2.2 and 2.3.  \n",
    "    - Visualize your results and interpret your results\n",
    "5. Use the word co-occurence matrix from Question 1. Compare the performance on the analogy task with the performance of your trained word embeddings.  \n",
    "6. Discuss:\n",
    "    - What are the main advantages of CBOW and Skipgram?\n",
    "    - What is the advantage of negative sampling?\n",
    "    - What are the main drawbacks of CBOW and Skipgram?\n",
    "7. Load pre-trained embeddings on large corpuses (see the pdf file). You only have to consider the word embeddings with an embedding size of 300\n",
    "    - Compare performance on the analogy task with your own trained embeddings from \"Alice in Wonderland\". You can limit yourself to the vocabulary of Alice in Wonderland. Visualize the pre-trained word embeddings and compare these with the results of your own trained word embeddings. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for cbow\n",
    "def generate_data_cbow(corpus, window_size, V):\n",
    "    all_in = [] \n",
    "    all_out = []\n",
    "    maxlen = window_size*2\n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            contexts = []\n",
    "            labels   = []            \n",
    "            s = index - window_size\n",
    "            e = index + window_size + 1\n",
    "            \n",
    "            contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
    "            labels.append(word)\n",
    "            if contexts != []:\n",
    "                all_in.append(sequence.pad_sequences(contexts, maxlen = maxlen))\n",
    "                all_out.append(np_utils.to_categorical(labels, V))\n",
    "    return (all_in,all_out) # return the values\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create CBOW model\n",
    "\n",
    "def getCbowModel(dim = 50, extra_layer = False, activation_extra_layer = 'softmax'):\n",
    "\n",
    "    # The Sequential model is a linear stack of layers.\n",
    "    cbow = Sequential()\n",
    "    cbow.add(Embedding(input_dim = V, output_dim = dim, input_length = window_size*2))\n",
    "\n",
    "    # TODO!!! \n",
    "    if extra_layer :\n",
    "        cbow.add(Dense(input_dim = V, units=dim, kernel_initializer='uniform', activation = activation_extra_layer))\n",
    "    \n",
    "    cbow.add( Lambda(lambda x: K.mean(x, axis = 1), output_shape = (dim,)) )\n",
    "    cbow.add( Dense(V, activation = 'softmax') )\n",
    "\n",
    "    # define loss function for Cbow\n",
    "    cbow.compile(loss='categorical_crossentropy', optimizer = 'adadelta')\n",
    "\n",
    "    return cbow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running CBOW (dim = 50)...\n",
      "0 41684.12801241875\n",
      "1 39103.43466985226\n",
      "--- 626.8190407752991 seconds ---\n",
      "Running CBOW (dim = 150)...\n",
      "0 41622.70653629303\n",
      "1 38772.33691751957\n",
      "--- 733.5874888896942 seconds ---\n",
      "Running CBOW (dim = 300)...\n",
      "0 41559.061150312424\n",
      "1 38503.73327308893\n",
      "--- 918.9292860031128 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "\n",
    "################################################################################\n",
    "############################ 1. Using CBOW model ###############################\n",
    "################################################################################\n",
    "\n",
    "dims = [50, 150, 300]\n",
    "iterations = 2\n",
    "\n",
    "for dimension in dims:\n",
    "    print(\"Running CBOW (dim = %s)...\"% dimension)\n",
    "    cbow = getCbowModel(dimension, False)\n",
    "    X, Y = generate_data_cbow(corpus, window_size, V)\n",
    "    for ite in range(iterations):\n",
    "        loss = 0.\n",
    "        for x, y in zip(X, Y):\n",
    "            loss += cbow.train_on_batch(x, y)\n",
    "        print(ite, loss)\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(\"--- %s seconds ---\" % (end_time - start_time))\n",
    "\n",
    "cbowWeights = cbow.get_weights()[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for Skipgram\n",
    "\n",
    "# This function creates a list where _each word_ in the text file (with non-empty content)\n",
    "# is encoded into two arrays:\n",
    "#   - x: an array of the size of all his windows_size neighbors with the value of the word (i.e, repeated values)\n",
    "#   - y: an array of the same size as x pointing to each neighbor. Finally encoded in a |V|-list as one-hot enc\n",
    "def generate_data_skipgram(corpus, window_size, V):\n",
    "    all_in = [] \n",
    "    all_out = []\n",
    "    for sentence in corpus: # For, sentence in corpus\n",
    "        L = len(sentence) # Limit for the iteration (per sentence)\n",
    "\n",
    "        # For each pair in the index\n",
    "        for index, word in enumerate(sentence):\n",
    "            # Create X and Y\n",
    "            in_words = [] # list of lists\n",
    "            labels = [] # list of integers\n",
    "\n",
    "            # Iterate over all the sentence, for each word in the line. I.e, for each word:\n",
    "            for i in range(index - window_size, index + window_size + 1):\n",
    "                # If between the limits and not myself\n",
    "                if i != index and (0 <= i < L):\n",
    "                    # This means: Associating the words in my neighbor (context) \n",
    "                    in_words.append([word]) # Current word (as a list) is associated to...\n",
    "                    labels.append(sentence[i]) # ... this word (as a simple integer).\n",
    "                    \n",
    "            # Once I have saved the context for the current word...\n",
    "            # ... if there is something in the arrays\n",
    "            if in_words != []:\n",
    "                # Save the information to the all_in/out arrays\n",
    "                all_in.append(np.array(in_words,dtype=np.int32)) # X in integer format\n",
    "                # Set Y to categorical values. If my output is ([6, 26]). Then I create\n",
    "                # a vector from 0 to V and mark 6 and 26 as 1\n",
    "                all_out.append(np_utils.to_categorical(labels, V))\n",
    "    return (all_in,all_out) # return the values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Skipgram model\n",
    "\n",
    "def getSkipgramModel(dim = 50, extra_layer = False, activation_extra_layer = 'softmax'):\n",
    "\n",
    "    # The Sequential model is a linear stack of layers.\n",
    "    skipgram = Sequential()\n",
    "    \n",
    "    # First layer (Projection layer. Embedding for wt)\n",
    "    # \n",
    "    # Parameters: \n",
    "    #   - input_dim: V (From the book: 1xV (input layer))\n",
    "    #   - embeddings_initializer = Glorot uniform. Is it for the uniform distribution of the noises?\n",
    "    #   - input_length: 1. It is needed for the dense layer (https://keras.io/layers/embeddings/#embedding)\n",
    "    # Input: One-hot ecoding of word wt in the 1xV space\n",
    "    # Output: vj in the dimension space 1xd\n",
    "    skipgram.add( Embedding(input_dim = V, output_dim = dim, embeddings_initializer = 'glorot_uniform', input_length = 1) )\n",
    "\n",
    "    # Intermediate layer. Can be seen as a helper only.\n",
    "    skipgram.add( Reshape((dim, )) )\n",
    "\n",
    "    \n",
    "    # Extra layer\n",
    "    #\n",
    "    # Justification: \n",
    "    #\n",
    "    #\n",
    "    if extra_layer :\n",
    "        skipgram.add(Dense(input_dim = dim, units = V, kernel_initializer = 'uniform', activation = activation_extra_layer))\n",
    "\n",
    "    # Last Layer (Output layer. Probabilities of context words)\n",
    "    # From documentation, Dense implements the operation: \n",
    "    #      output = activation(dot(input, kernel) + bias) \n",
    "    #   where:\n",
    "    #      activation is the element-wise activation function passed as the activation argument\n",
    "    #      kernel is a weights matrix created by the layer\n",
    "    #      and bias is a bias vector created by the layer (only applicable if use_bias is True). _WE DO NOT SET THIS_.\n",
    "    #\n",
    "    # Hence, we are computing ck*vj\n",
    "    #\n",
    "    # Parameters:\n",
    "    #   - input_dim: dim. From embedding layer, dx1\n",
    "    #   - units: V. The output dimension, From the book 1xV\n",
    "    #   - kernel_initializer: uniform (?)\n",
    "    #   - activation: softmax (as described in the book)\n",
    "\n",
    "    skipgram.add( Dense(input_dim = dim, units = V, kernel_initializer = 'uniform', activation = 'softmax') )\n",
    "    \n",
    "    \n",
    "    # define loss function for Skipgram\n",
    "    skipgram.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "\n",
    "    return skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Skipgram (dim = 50)...\n",
      "0 41271.35414361954\n",
      "1 39126.83639883995\n",
      "2 39288.24052977562\n",
      "3 39363.89170718193\n",
      "4 39432.41858673096\n",
      "5 39511.995033741\n",
      "6 39608.390723228455\n",
      "7 39723.89541876316\n",
      "--- 153.39438009262085 seconds ---\n",
      "Running Skipgram (dim = 150)...\n",
      "0 41222.66726255417\n",
      "1 38923.405826091766\n",
      "2 38988.34033322334\n",
      "3 39005.65833210945\n",
      "4 39038.85262310505\n",
      "5 39093.713930249214\n",
      "6 39156.146334052086\n",
      "7 39216.637645840645\n",
      "--- 323.76494121551514 seconds ---\n",
      "Running Skipgram (dim = 300)...\n",
      "0 41159.68821120262\n",
      "1 38730.843128442764\n",
      "2 38713.53500235081\n",
      "3 38676.768409490585\n",
      "4 38648.03497707844\n",
      "5 38631.883412480354\n",
      "6 38611.77797353268\n",
      "7 38578.92355453968\n",
      "--- 637.9695401191711 seconds ---\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "########################## 2. Using Skipgram model #############################\n",
    "################################################################################\n",
    "\n",
    "dims = [50, 150, 300]\n",
    "iterations = 8\n",
    "\n",
    "for dimension in dims:\n",
    "    print(\"Running Skipgram (dim = %s)...\"% dimension)\n",
    "    skipgram = getSkipgramModel(dimension, False)\n",
    "    #print(skipgram.summary())\n",
    "    X, Y = generate_data_skipgram(corpus, window_size, V)\n",
    "\n",
    "    start_time = time.time()\n",
    "    for ite in range(iterations):\n",
    "        loss = 0.\n",
    "        for x, y in zip(X,Y):\n",
    "            loss += skipgram.train_on_batch(x, y)\n",
    "        print(ite, loss)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(\"--- %s seconds ---\" % (end_time - start_time))\n",
    "    \n",
    "skipgramWeights = skipgram.get_weights()[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW additional hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Skipgram (dim = 50) with activation rule 'softmax'...\n",
      "0 41624.25065732002\n",
      "1 39531.59989595413\n",
      "--- 49.01548528671265 seconds ---\n",
      "Running Skipgram (dim = 50) with activation rule 'elu'...\n",
      "0 40056.498475790024\n",
      "1 38624.99711847305\n",
      "--- 34.012505769729614 seconds ---\n",
      "Running Skipgram (dim = 50) with activation rule 'selu'...\n",
      "0 39877.45523810387\n",
      "1 38574.679033041\n",
      "--- 33.73241710662842 seconds ---\n",
      "Running Skipgram (dim = 50) with activation rule 'softplus'...\n",
      "0 39924.52046144009\n",
      "1 39497.91260719299\n",
      "--- 38.39311408996582 seconds ---\n",
      "Running Skipgram (dim = 50) with activation rule 'softsign'...\n",
      "0 40358.41834950447\n",
      "1 38613.81048035622\n",
      "--- 38.05812382698059 seconds ---\n",
      "Running Skipgram (dim = 50) with activation rule 'relu'...\n",
      "0 40086.01563549042\n",
      "1 39067.42638683319\n",
      "--- 38.54331088066101 seconds ---\n",
      "Running Skipgram (dim = 50) with activation rule 'tanh'...\n",
      "0 40136.364102602005\n",
      "1 38636.88547039032\n",
      "--- 38.76637101173401 seconds ---\n",
      "Running Skipgram (dim = 50) with activation rule 'sigmoid'...\n",
      "0 40000.60475945473\n",
      "1 39515.73431766033\n",
      "--- 38.342857122421265 seconds ---\n",
      "Running Skipgram (dim = 50) with activation rule 'hard_sigmoid'...\n",
      "0 40029.409319996834\n",
      "1 39454.99852800369\n",
      "--- 43.21289086341858 seconds ---\n",
      "Running Skipgram (dim = 50) with activation rule 'linear'...\n",
      "0 39926.36301767826\n",
      "1 38698.675266861916\n",
      "--- 38.37696099281311 seconds ---\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "################### 3. Using Skipgram model (extra layer) ######################\n",
    "################################################################################\n",
    "\n",
    "#dims = [50, 150, 300]\n",
    "dims = [50]\n",
    "iterations = 2\n",
    "\n",
    "activation_rules = [\"softmax\", \"elu\", \"selu\", \"softplus\", \"softsign\", \"relu\", \"tanh\", \"sigmoid\", \"hard_sigmoid\", \"linear\"]\n",
    "for dimension in dims:\n",
    "    for activation_rule in activation_rules:\n",
    "        print(\"Running Skipgram (dim = %s) with activation rule '%s'...\"% (dimension, activation_rule))\n",
    "        try:\n",
    "            # set extra_layer = true\n",
    "            cbow = getCbowModel(dimension, True, activation_rule)\n",
    "            X, Y = generate_data_cbow(corpus, window_size, V)\n",
    "\n",
    "            start_time = time.time()\n",
    "            for ite in range(iterations):\n",
    "                loss = 0.\n",
    "                for x, y in zip(X,Y):\n",
    "                    loss += cbow.train_on_batch(x, y)\n",
    "                print(ite, loss)\n",
    "            end_time = time.time()\n",
    "\n",
    "            print(\"--- %s seconds ---\" % (end_time - start_time))\n",
    "        except:\n",
    "            print(\"Error with\", activation_rule)\n",
    "\n",
    "cbowHiddenLayerWeights = cbow.get_weights()[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skipgram additional hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Skipgram (dim = 50) with activation rule 'softmax'...\n",
      "0 41252.48097372055\n",
      "1 39351.12030816078\n",
      "--- 358.6214940547943 seconds ---\n",
      "Running Skipgram (dim = 50) with activation rule 'elu'...\n",
      "0 38090.17261695862\n",
      "1 37339.52349615097\n",
      "--- 360.26901602745056 seconds ---\n",
      "Running Skipgram (dim = 50) with activation rule 'selu'...\n",
      "0 37969.02503442764\n",
      "1 37144.54713857174\n",
      "--- 355.1335310935974 seconds ---\n",
      "Running Skipgram (dim = 50) with activation rule 'softplus'...\n",
      "0 36149.578640937805\n",
      "1 37012.700358986855\n",
      "--- 354.5474681854248 seconds ---\n",
      "Running Skipgram (dim = 50) with activation rule 'softsign'...\n",
      "0 38161.69445538521\n",
      "1 37472.23338055611\n",
      "--- 355.6916038990021 seconds ---\n",
      "Running Skipgram (dim = 50) with activation rule 'relu'...\n",
      "0 38548.03942465782\n",
      "1 38201.609553813934\n",
      "--- 402.5509879589081 seconds ---\n",
      "Running Skipgram (dim = 50) with activation rule 'tanh'...\n",
      "0 38081.362594127655\n",
      "1 37329.15055298805\n",
      "--- 390.59892201423645 seconds ---\n",
      "Running Skipgram (dim = 50) with activation rule 'sigmoid'...\n",
      "0 36598.57139098644\n",
      "1 37205.09107923508\n",
      "--- 399.11798095703125 seconds ---\n",
      "Running Skipgram (dim = 50) with activation rule 'hard_sigmoid'...\n",
      "0 36548.80845117569\n",
      "1 37142.62876391411\n",
      "--- 389.4538559913635 seconds ---\n",
      "Running Skipgram (dim = 50) with activation rule 'linear'...\n",
      "0 38075.388264894485\n",
      "1 37336.66932654381\n",
      "--- 396.0674366950989 seconds ---\n",
      "Running Skipgram (dim = 150) with activation rule 'softmax'...\n",
      "0 41252.39979863167\n",
      "1 39351.147944927216\n",
      "--- 446.76273703575134 seconds ---\n",
      "Running Skipgram (dim = 150) with activation rule 'elu'...\n",
      "0 38033.09181690216\n",
      "1 37079.43993103504\n",
      "--- 447.79997086524963 seconds ---\n",
      "Running Skipgram (dim = 150) with activation rule 'selu'...\n",
      "0 37917.4889036417\n",
      "1 36874.7311309576\n",
      "--- 451.5342059135437 seconds ---\n",
      "Running Skipgram (dim = 150) with activation rule 'softplus'...\n",
      "0 36137.83169424534\n",
      "1 37002.82253026962\n",
      "--- 444.891389131546 seconds ---\n",
      "Running Skipgram (dim = 150) with activation rule 'softsign'...\n",
      "0 38102.49917435646\n",
      "1 37182.390253186226\n",
      "--- 449.789834022522 seconds ---\n",
      "Running Skipgram (dim = 150) with activation rule 'relu'...\n",
      "0 38529.661886930466\n",
      "1 38240.970066189766\n",
      "--- 448.00920605659485 seconds ---\n",
      "Running Skipgram (dim = 150) with activation rule 'tanh'...\n",
      "0 38048.13074970245\n",
      "1 37081.09804880619\n",
      "--- 450.14839720726013 seconds ---\n",
      "Running Skipgram (dim = 150) with activation rule 'sigmoid'...\n",
      "0 36586.37559103966\n",
      "1 37211.570682406425\n",
      "--- 442.7454059123993 seconds ---\n",
      "Running Skipgram (dim = 150) with activation rule 'hard_sigmoid'...\n",
      "0 36541.065417289734\n",
      "1 37188.25612556934\n",
      "--- 449.2917432785034 seconds ---\n",
      "Running Skipgram (dim = 150) with activation rule 'linear'...\n",
      "0 38036.608541965485\n",
      "1 37034.985434532166\n",
      "--- 445.3898620605469 seconds ---\n",
      "Running Skipgram (dim = 300) with activation rule 'softmax'...\n",
      "0 41251.708943367004\n",
      "1 39351.26709794998\n",
      "--- 513.8344006538391 seconds ---\n",
      "Running Skipgram (dim = 300) with activation rule 'elu'...\n",
      "0 38013.69291305542\n",
      "1 36798.870677948\n",
      "--- 518.2157430648804 seconds ---\n",
      "Running Skipgram (dim = 300) with activation rule 'selu'...\n",
      "0 37908.72470498085\n",
      "1 36514.12233161926\n",
      "--- 513.1629691123962 seconds ---\n",
      "Running Skipgram (dim = 300) with activation rule 'softplus'...\n",
      "0 36147.637127399445\n",
      "1 36940.78172492981\n",
      "--- 515.9760899543762 seconds ---\n",
      "Running Skipgram (dim = 300) with activation rule 'softsign'...\n",
      "0 38070.89290547371\n",
      "1 36932.402485489845\n",
      "--- 508.09692788124084 seconds ---\n",
      "Running Skipgram (dim = 300) with activation rule 'relu'...\n",
      "0 38545.92240834236\n",
      "1 38356.3127361536\n",
      "--- 533.2793619632721 seconds ---\n",
      "Running Skipgram (dim = 300) with activation rule 'tanh'...\n",
      "0 37995.81114530563\n",
      "1 36789.76421737671\n",
      "--- 522.5242700576782 seconds ---\n",
      "Running Skipgram (dim = 300) with activation rule 'sigmoid'...\n",
      "0 36588.66371464729\n",
      "1 37195.522879600525\n",
      "--- 523.505108833313 seconds ---\n",
      "Running Skipgram (dim = 300) with activation rule 'hard_sigmoid'...\n",
      "0 36541.11595225334\n",
      "1 37170.17481994629\n",
      "--- 517.9205141067505 seconds ---\n",
      "Running Skipgram (dim = 300) with activation rule 'linear'...\n",
      "0 38006.789974331856\n",
      "1 36784.10951447487\n",
      "--- 520.0513091087341 seconds ---\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "################### 3. Using Skipgram model (extra layer) ######################\n",
    "################################################################################\n",
    "\n",
    "dims = [50, 150, 300]\n",
    "#dims = [50]\n",
    "iterations = 2\n",
    "\n",
    "activation_rules = [\"softmax\", \"elu\", \"selu\", \"softplus\", \"softsign\", \"relu\", \"tanh\", \"sigmoid\", \"hard_sigmoid\", \"linear\"]\n",
    "for dimension in dims:\n",
    "    for activation_rule in activation_rules:\n",
    "        print(\"Running Skipgram (dim = %s) with activation rule '%s'...\"% (dimension, activation_rule))\n",
    "        try:\n",
    "            # set extra_layer = true\n",
    "            skipgram = getSkipgramModel(dimension, True, activation_rule)\n",
    "            X, Y = generate_data_skipgram(corpus, window_size, V)\n",
    "\n",
    "            start_time = time.time()\n",
    "            for ite in range(iterations):\n",
    "                loss = 0.\n",
    "                for x, y in zip(X,Y):\n",
    "                    loss += skipgram.train_on_batch(x, y)\n",
    "                print(ite, loss)\n",
    "            end_time = time.time()\n",
    "\n",
    "            print(\"--- %s seconds ---\" % (end_time - start_time))\n",
    "        except:\n",
    "            print(\"Error with\", activation_rule)\n",
    "\n",
    "skipgramHiddenLayerWeights = skipgram.get_weights()[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08406705,  0.09400582,  0.08651898,  0.07330547,  0.01389818,\n",
       "        0.05978406,  0.0272817 ,  0.00520233, -0.00388816,  0.04627812,\n",
       "       -0.03466139, -0.05365833, -0.0472076 ,  0.05588843,  0.08260176,\n",
       "        0.05304934,  0.04205545,  0.00162497, -0.03770801,  0.01696711,\n",
       "       -0.04566567,  0.03415297, -0.02130879,  0.073093  , -0.05127802,\n",
       "        0.01507935,  0.05303619, -0.038434  , -0.02429472, -0.03786976,\n",
       "       -0.04851353, -0.00866184,  0.04393646,  0.05953187, -0.01863442,\n",
       "       -0.02132674,  0.02127859,  0.01419815,  0.07605583, -0.02545334,\n",
       "        0.01741531,  0.05263753,  0.02294727,  0.02463048, -0.00261251,\n",
       "        0.03773098,  0.02235954,  0.02556502, -0.00441569,  0.01598156,\n",
       "       -0.05385536,  0.0299219 ,  0.00551344, -0.02750573,  0.05955534,\n",
       "        0.05302291,  0.04511306,  0.01793033, -0.02903962, -0.07581343,\n",
       "        0.02622947, -0.02973812,  0.00726978,  0.02505386,  0.01324776,\n",
       "        0.02039069,  0.08872194,  0.04100237,  0.06498899, -0.0383313 ,\n",
       "       -0.03277691,  0.00138467,  0.03032126, -0.00823926,  0.02743659,\n",
       "       -0.01167131, -0.02082407,  0.00311961, -0.02948995, -0.03241683,\n",
       "       -0.03421885, -0.01726916,  0.00365435, -0.07507385, -0.03715593,\n",
       "       -0.05653462,  0.00122755, -0.02731076,  0.0282659 , -0.07879315,\n",
       "        0.03616068,  0.02775585, -0.03877775,  0.01570722,  0.03587529,\n",
       "       -0.00766357, -0.01247033,  0.01992456, -0.00972898,  0.00467202,\n",
       "        0.03217654,  0.00266737, -0.03444177, -0.05467112, -0.00802116,\n",
       "       -0.05414046,  0.09868632, -0.04057125, -0.01402971, -0.00046526,\n",
       "        0.04417517, -0.03395222, -0.0732889 ,  0.00232122,  0.05985306,\n",
       "       -0.02287834,  0.00807971,  0.0159425 , -0.00581036, -0.01098365,\n",
       "        0.09251309,  0.00869817,  0.02047425,  0.00745478, -0.03322984,\n",
       "        0.04011598,  0.01015033, -0.08361985,  0.03791481, -0.04409946,\n",
       "        0.02991512,  0.00390361,  0.00322622, -0.02009575, -0.04541248,\n",
       "        0.03359366, -0.02298874, -0.00619026,  0.02256676, -0.00789292,\n",
       "        0.02618397,  0.03185496, -0.07040073,  0.02042732, -0.002514  ,\n",
       "       -0.04928241,  0.03112379,  0.00559432, -0.01834989, -0.02272663,\n",
       "        0.0401807 ,  0.00558569, -0.03935769, -0.0559916 ,  0.10027555,\n",
       "        0.03771575,  0.01212556,  0.05938905,  0.00420855, -0.00081375,\n",
       "       -0.00061226, -0.02178224, -0.02198534,  0.10445353,  0.08625517,\n",
       "        0.01278338, -0.03469757, -0.00498733, -0.04020981, -0.02036622,\n",
       "        0.01977624,  0.01636548,  0.0010405 , -0.00906629, -0.01347195,\n",
       "        0.0453441 ,  0.00298213,  0.0244512 ,  0.01011757,  0.02393248,\n",
       "        0.03419012, -0.01224271,  0.06645326, -0.01671716,  0.00699815,\n",
       "       -0.01548025, -0.06206299,  0.0030923 , -0.01226318,  0.036545  ,\n",
       "       -0.01945529, -0.03954564,  0.041125  ,  0.05558563,  0.0340717 ,\n",
       "       -0.01357647,  0.01342965, -0.05442015, -0.053223  , -0.01123583,\n",
       "        0.01521801, -0.01990578, -0.00400044,  0.08396815,  0.01593211,\n",
       "       -0.00241406,  0.0480281 , -0.1054189 ,  0.0336827 ,  0.02730022,\n",
       "        0.01528443,  0.0458713 , -0.04942683, -0.053614  ,  0.08089906,\n",
       "       -0.08765722,  0.0449305 , -0.06073828, -0.00785655, -0.00277843,\n",
       "       -0.03477402,  0.02201466,  0.04259675, -0.03473526, -0.03339678,\n",
       "       -0.02031064, -0.02554632,  0.00386602,  0.03130088,  0.00239548,\n",
       "        0.02443099,  0.02238559, -0.09331778, -0.00409476, -0.00689682,\n",
       "        0.06445175, -0.05260758, -0.01936094,  0.05225708,  0.03749912,\n",
       "        0.03074867, -0.01300159,  0.0443057 , -0.0419805 , -0.0137604 ,\n",
       "        0.05443529,  0.08105616, -0.06236884,  0.04951951, -0.02018141,\n",
       "       -0.01355736, -0.01893969,  0.02978799,  0.01308109, -0.01025374,\n",
       "       -0.07800598, -0.02890588, -0.00626145, -0.04052567, -0.02835203,\n",
       "        0.03345836, -0.03592227, -0.11892393,  0.03762184, -0.01774433,\n",
       "        0.01677299,  0.04328488,  0.00033926, -0.03829532, -0.05616987,\n",
       "       -0.00410957, -0.00178822, -0.03637673, -0.01402603,  0.05919661,\n",
       "       -0.00074399,  0.05980977,  0.03408968, -0.01545197, -0.01407751,\n",
       "       -0.05532463,  0.03619134,  0.04449244,  0.032746  , -0.01408981,\n",
       "       -0.00675055, -0.05239393, -0.03721964, -0.00601298, -0.02363475,\n",
       "        0.05128881, -0.05146854,  0.02947038, -0.00831717,  0.1144554 ,\n",
       "       -0.00341013, -0.05082112, -0.03918894,  0.05460871, -0.04805864],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgramHiddenLayerWeights.shape\n",
    "skipgramHiddenLayerWeights[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "####### 4a. Implement your own function to perform the analogy task with #######\n",
    "################################################################################\n",
    "\n",
    "EMBEDDING_TYPE_1 = \"wcoMatrix\"\n",
    "EMBEDDING_TYPE_2 = \"distributed\"\n",
    "EMBEDDING_TYPE_4 = \"word2vec\"\n",
    "\n",
    "\n",
    "# neighbors is the NearestNeighbor object containing the computed neighbors matrix\n",
    "# objA is to objB as objC is a objD\n",
    "# we assume objs are strings\n",
    "def analogyFunction(neighbors, index, objA, objB, objC, objD, embeddingType = EMBEDDING_TYPE_1, matrix = wcoMatrix):\n",
    "    \n",
    "    if embeddingType != EMBEDDING_TYPE_4:\n",
    "        indexA = index[objA]\n",
    "        indexB = index[objB]\n",
    "        indexC = index[objC]\n",
    "        indexD = index[objD]\n",
    "    \n",
    "    if embeddingType == EMBEDDING_TYPE_1 or embeddingType == EMBEDDING_TYPE_2:\n",
    "        vecA = np.asarray(matrix[indexA,])\n",
    "        vecB = np.asarray(matrix[indexB,])\n",
    "        vecC = np.asarray(matrix[indexC,])\n",
    "        vecD = np.asarray(matrix[indexD,])\n",
    "    \n",
    "    if embeddingType == EMBEDDING_TYPE_4 :\n",
    "        vecA = index.get_vector(objA)\n",
    "        vecB = index.get_vector(objB)\n",
    "        vecC = index.get_vector(objC)\n",
    "        vecD = index.get_vector(objD)\n",
    "\n",
    "    operationResult = vecA - vecB + vecD\n",
    "    \n",
    "    # If the word completely matches, return true\n",
    "    if np.array_equal(operationResult, vecC) :\n",
    "        return True\n",
    "    \n",
    "    # Otherwise, get the nearest neighbor of the resulting vector\n",
    "    if embeddingType == EMBEDDING_TYPE_4 :\n",
    "        finalWord = index.similar_by_vector(operationResult, 1)[0][0]\n",
    "    else:\n",
    "        distances, indices = neighbors.kneighbors([operationResult])\n",
    "        neighborIndex = indices[0, 0]\n",
    "        finalWord = getKeyFromValue(index, neighborIndex)\n",
    "\n",
    "    print(\"\\tComparing %s vs. %s (word vs. prediction)\" % (objC, finalWord))\n",
    "\n",
    "    # Return comparison of words\n",
    "    return ( objC == finalWord )\n",
    "\n",
    "\n",
    "def makeAnalogyOnFile(neighbors, embeddingType = EMBEDDING_TYPE_1, matrix = wcoMatrix, index = full_index):\n",
    "    # Read file\n",
    "    content = []\n",
    "    with open(\"analogy_alice.txt\") as f:\n",
    "        content = f.readlines()\n",
    "\n",
    "    # Remove whitespace characters like '\\n' at the end of each line\n",
    "    content = [x.strip() for x in content] \n",
    "    allResults = []\n",
    "    \n",
    "    for analogy in content:\n",
    "        words = analogy.split(\" \")\n",
    "        if len(words) == 4:\n",
    "            try:\n",
    "                result = analogyFunction(neighbors, index, words[0], words[1], words[2], words[3], embeddingType, matrix)\n",
    "                allResults.append(result)\n",
    "                print(\"'%s': %s\" % (analogy, result))\n",
    "            except KeyError:\n",
    "                print(\"Missing words in the index \")\n",
    "    return allResults\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing words in the index \n",
      "\tComparing usual vs. sudden (word vs. prediction)\n",
      "'sudden suddenly usual usually': False\n",
      "\tComparing good vs. better (word vs. prediction)\n",
      "'bad worse good better': False\n",
      "\tComparing look vs. looking (word vs. prediction)\n",
      "'go going look looking': False\n",
      "\tComparing his vs. her (word vs. prediction)\n",
      "'he she his her': False\n",
      "\tComparing his vs. her (word vs. prediction)\n",
      "'brother sister his her': False\n",
      "\tComparing look vs. looking (word vs. prediction)\n",
      "'listen listening look looking': False\n",
      "\tComparing thinking vs. saying (word vs. prediction)\n",
      "'saying said thinking thought': False\n",
      "\tComparing cat vs. cats (word vs. prediction)\n",
      "'bird birds cat cats': False\n",
      "\tComparing old vs. good (word vs. prediction)\n",
      "'good better old older': False\n",
      "\tComparing quick vs. good (word vs. prediction)\n",
      "'good better quick quicker': False\n",
      "\tComparing good vs. large (word vs. prediction)\n",
      "'large largest good best': False\n",
      "Missing words in the index \n",
      "\tComparing knowing vs. falling (word vs. prediction)\n",
      "'falling fell knowing knew': False\n",
      "\tComparing think vs. thinking (word vs. prediction)\n",
      "'walk walking think thinking': False\n",
      "\tComparing cat vs. cats (word vs. prediction)\n",
      "'child children cat cats': False\n",
      "\tComparing eye vs. dog (word vs. prediction)\n",
      "'dog dogs eye eyes': False\n",
      "\tComparing rat vs. hand (word vs. prediction)\n",
      "'hand hands rat rats': False\n",
      "\tComparing find vs. eat (word vs. prediction)\n",
      "'eat eats find finds': False\n",
      "\tComparing say vs. find (word vs. prediction)\n",
      "'find finds say says': False\n",
      "\tComparing good vs. old (word vs. prediction)\n",
      "'old older good better': False\n",
      "\tComparing quick vs. large (word vs. prediction)\n",
      "'large larger quick quicker': False\n",
      "\tComparing listen vs. listening (word vs. prediction)\n",
      "'go going listen listening': False\n",
      "\tComparing walk vs. walking (word vs. prediction)\n",
      "'run running walk walking': False\n",
      "\tComparing think vs. thinking (word vs. prediction)\n",
      "'run running think thinking': False\n",
      "Missing words in the index \n",
      "\tComparing sit vs. say (word vs. prediction)\n",
      "'say saying sit sitting': False\n",
      "Missing words in the index \n",
      "\tComparing rabbit vs. burnt (word vs. prediction)\n",
      "'alice she rabbit he': False\n",
      "\tComparing rabbit vs. alice (word vs. prediction)\n",
      "'alice her rabbit him': False\n",
      "\tComparing rabbit vs. alice (word vs. prediction)\n",
      "'alice girl rabbit sir': False\n",
      "Missing words in the index \n",
      "Missing words in the index \n",
      "\tComparing alice vs. dinah (word vs. prediction)\n",
      "'dinah cat alice girl': False\n",
      "Missing words in the index \n",
      "\tComparing he vs. she (word vs. prediction)\n",
      "'his her he she': False\n",
      "\tComparing quick vs. long (word vs. prediction)\n",
      "'long longer quick quicker': False\n",
      "\tComparing small vs. long (word vs. prediction)\n",
      "'long longer small smaller': False\n",
      "\tComparing bad vs. long (word vs. prediction)\n",
      "'long longer bad worse': False\n",
      "\tComparing look vs. looking (word vs. prediction)\n",
      "'go going look looking': False\n",
      "\tComparing look vs. looking (word vs. prediction)\n",
      "'listen listening look looking': False\n",
      "\tComparing sit vs. swim (word vs. prediction)\n",
      "'swim swimming sit sitting': False\n",
      "\tComparing listen vs. listening (word vs. prediction)\n",
      "'run running listen listening': False\n",
      "\tComparing read vs. think (word vs. prediction)\n",
      "'think thinking read reading': False\n",
      "\tComparing close vs. up (word vs. prediction)\n",
      "'up down close far': False\n",
      "Missing words in the index \n",
      "Missing words in the index \n",
      "Score for skipgram embedding: 0\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "########### 4b. Compare the performance on the analogy task between ############\n",
    "########### the word embeddings you have trained in 2.1, 2.2 & 2.3  ############\n",
    "################################################################################\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors = 1).fit(skipgramWeights)\n",
    "analogyResultsSkipgram1 = makeAnalogyOnFile(nbrs, EMBEDDING_TYPE_2, skipgramWeights, full_index)\n",
    "print(\"Score for skipgram embedding: %s\" % sum(analogyResultsSkipgram1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing words in the index \n",
      "\tComparing usual vs. sudden (word vs. prediction)\n",
      "'sudden suddenly usual usually': False\n",
      "\tComparing good vs. better (word vs. prediction)\n",
      "'bad worse good better': False\n",
      "\tComparing look vs. looking (word vs. prediction)\n",
      "'go going look looking': False\n",
      "\tComparing his vs. with (word vs. prediction)\n",
      "'he she his her': False\n",
      "\tComparing his vs. her (word vs. prediction)\n",
      "'brother sister his her': False\n",
      "\tComparing look vs. listen (word vs. prediction)\n",
      "'listen listening look looking': False\n",
      "\tComparing thinking vs. saying (word vs. prediction)\n",
      "'saying said thinking thought': False\n",
      "\tComparing cat vs. cats (word vs. prediction)\n",
      "'bird birds cat cats': False\n",
      "\tComparing old vs. good (word vs. prediction)\n",
      "'good better old older': False\n",
      "\tComparing quick vs. good (word vs. prediction)\n",
      "'good better quick quicker': False\n",
      "\tComparing good vs. large (word vs. prediction)\n",
      "'large largest good best': False\n",
      "Missing words in the index \n",
      "\tComparing knowing vs. knew (word vs. prediction)\n",
      "'falling fell knowing knew': False\n",
      "\tComparing think vs. walk (word vs. prediction)\n",
      "'walk walking think thinking': False\n",
      "\tComparing cat vs. cats (word vs. prediction)\n",
      "'child children cat cats': False\n",
      "\tComparing eye vs. eyes (word vs. prediction)\n",
      "'dog dogs eye eyes': False\n",
      "\tComparing rat vs. hand (word vs. prediction)\n",
      "'hand hands rat rats': False\n",
      "\tComparing find vs. eat (word vs. prediction)\n",
      "'eat eats find finds': False\n",
      "\tComparing say vs. says (word vs. prediction)\n",
      "'find finds say says': False\n",
      "\tComparing good vs. better (word vs. prediction)\n",
      "'old older good better': False\n",
      "\tComparing quick vs. large (word vs. prediction)\n",
      "'large larger quick quicker': False\n",
      "\tComparing listen vs. go (word vs. prediction)\n",
      "'go going listen listening': False\n",
      "\tComparing walk vs. walking (word vs. prediction)\n",
      "'run running walk walking': False\n",
      "\tComparing think vs. thinking (word vs. prediction)\n",
      "'run running think thinking': False\n",
      "Missing words in the index \n",
      "\tComparing sit vs. say (word vs. prediction)\n",
      "'say saying sit sitting': False\n",
      "Missing words in the index \n",
      "\tComparing rabbit vs. speak (word vs. prediction)\n",
      "'alice she rabbit he': False\n",
      "\tComparing rabbit vs. alice (word vs. prediction)\n",
      "'alice her rabbit him': False\n",
      "\tComparing rabbit vs. alice (word vs. prediction)\n",
      "'alice girl rabbit sir': False\n",
      "Missing words in the index \n",
      "Missing words in the index \n",
      "\tComparing alice vs. dinah (word vs. prediction)\n",
      "'dinah cat alice girl': False\n",
      "Missing words in the index \n",
      "\tComparing he vs. she (word vs. prediction)\n",
      "'his her he she': False\n",
      "\tComparing quick vs. long (word vs. prediction)\n",
      "'long longer quick quicker': False\n",
      "\tComparing small vs. long (word vs. prediction)\n",
      "'long longer small smaller': False\n",
      "\tComparing bad vs. long (word vs. prediction)\n",
      "'long longer bad worse': False\n",
      "\tComparing look vs. looking (word vs. prediction)\n",
      "'go going look looking': False\n",
      "\tComparing look vs. listen (word vs. prediction)\n",
      "'listen listening look looking': False\n",
      "\tComparing sit vs. swim (word vs. prediction)\n",
      "'swim swimming sit sitting': False\n",
      "\tComparing listen vs. listening (word vs. prediction)\n",
      "'run running listen listening': False\n",
      "\tComparing read vs. think (word vs. prediction)\n",
      "'think thinking read reading': False\n",
      "\tComparing close vs. up (word vs. prediction)\n",
      "'up down close far': False\n",
      "Missing words in the index \n",
      "Missing words in the index \n",
      "Score for cbow embedding: 0\n"
     ]
    }
   ],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors = 1).fit(cbowWeights)\n",
    "analogyResultsSkipgram1 = makeAnalogyOnFile(nbrs, EMBEDDING_TYPE_2, cbowWeights, full_index)\n",
    "print(\"Score for cbow embedding: %s\" % sum(analogyResultsSkipgram1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing words in the index \n",
      "\tComparing usual vs. usually (word vs. prediction)\n",
      "'sudden suddenly usual usually': False\n",
      "\tComparing good vs. better (word vs. prediction)\n",
      "'bad worse good better': False\n",
      "\tComparing look vs. looking (word vs. prediction)\n",
      "'go going look looking': False\n",
      "\tComparing his vs. her (word vs. prediction)\n",
      "'he she his her': False\n",
      "\tComparing his vs. her (word vs. prediction)\n",
      "'brother sister his her': False\n",
      "\tComparing look vs. looking (word vs. prediction)\n",
      "'listen listening look looking': False\n",
      "\tComparing thinking vs. saying (word vs. prediction)\n",
      "'saying said thinking thought': False\n",
      "\tComparing cat vs. cats (word vs. prediction)\n",
      "'bird birds cat cats': False\n",
      "\tComparing old vs. good (word vs. prediction)\n",
      "'good better old older': False\n",
      "\tComparing quick vs. good (word vs. prediction)\n",
      "'good better quick quicker': False\n",
      "\tComparing good vs. large (word vs. prediction)\n",
      "'large largest good best': False\n",
      "Missing words in the index \n",
      "\tComparing knowing vs. falling (word vs. prediction)\n",
      "'falling fell knowing knew': False\n",
      "\tComparing think vs. walk (word vs. prediction)\n",
      "'walk walking think thinking': False\n",
      "\tComparing cat vs. cats (word vs. prediction)\n",
      "'child children cat cats': False\n",
      "\tComparing eye vs. dog (word vs. prediction)\n",
      "'dog dogs eye eyes': False\n",
      "\tComparing rat vs. rats (word vs. prediction)\n",
      "'hand hands rat rats': False\n",
      "\tComparing find vs. finds (word vs. prediction)\n",
      "'eat eats find finds': False\n",
      "\tComparing say vs. says (word vs. prediction)\n",
      "'find finds say says': False\n",
      "\tComparing good vs. old (word vs. prediction)\n",
      "'old older good better': False\n",
      "\tComparing quick vs. quicker (word vs. prediction)\n",
      "'large larger quick quicker': False\n",
      "\tComparing listen vs. listening (word vs. prediction)\n",
      "'go going listen listening': False\n",
      "\tComparing walk vs. run (word vs. prediction)\n",
      "'run running walk walking': False\n",
      "\tComparing think vs. run (word vs. prediction)\n",
      "'run running think thinking': False\n",
      "Missing words in the index \n",
      "\tComparing sit vs. sitting (word vs. prediction)\n",
      "'say saying sit sitting': False\n",
      "Missing words in the index \n",
      "\tComparing rabbit vs. he (word vs. prediction)\n",
      "'alice she rabbit he': False\n",
      "\tComparing rabbit vs. him (word vs. prediction)\n",
      "'alice her rabbit him': False\n",
      "\tComparing rabbit vs. alice (word vs. prediction)\n",
      "'alice girl rabbit sir': False\n",
      "Missing words in the index \n",
      "Missing words in the index \n",
      "\tComparing alice vs. girl (word vs. prediction)\n",
      "'dinah cat alice girl': False\n",
      "Missing words in the index \n",
      "\tComparing he vs. his (word vs. prediction)\n",
      "'his her he she': False\n",
      "\tComparing quick vs. long (word vs. prediction)\n",
      "'long longer quick quicker': False\n",
      "\tComparing small vs. smaller (word vs. prediction)\n",
      "'long longer small smaller': False\n",
      "\tComparing bad vs. worse (word vs. prediction)\n",
      "'long longer bad worse': False\n",
      "\tComparing look vs. looking (word vs. prediction)\n",
      "'go going look looking': False\n",
      "\tComparing look vs. looking (word vs. prediction)\n",
      "'listen listening look looking': False\n",
      "\tComparing sit vs. sitting (word vs. prediction)\n",
      "'swim swimming sit sitting': False\n",
      "\tComparing listen vs. listening (word vs. prediction)\n",
      "'run running listen listening': False\n",
      "\tComparing read vs. think (word vs. prediction)\n",
      "'think thinking read reading': False\n",
      "\tComparing close vs. up (word vs. prediction)\n",
      "'up down close far': False\n",
      "Missing words in the index \n",
      "Missing words in the index \n",
      "Score for skipgram (extra hideen layer) embedding: 0\n"
     ]
    }
   ],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors = 1).fit(skipgramHiddenLayerWeights)\n",
    "analogyResultsSkipgram1 = makeAnalogyOnFile(nbrs, EMBEDDING_TYPE_2, skipgramHiddenLayerWeights, full_index)\n",
    "print(\"Score for skipgram (extra hideen layer) embedding: %s\" % sum(analogyResultsSkipgram1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cbowHiddenLayerWeights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-198-d1d74570d7b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnbrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbowHiddenLayerWeights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0manalogyResultsSkipgram1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmakeAnalogyOnFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBEDDING_TYPE_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbowHiddenLayerWeights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Score for cbow (extra hideen layer) embedding: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalogyResultsSkipgram1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cbowHiddenLayerWeights' is not defined"
     ]
    }
   ],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors = 1).fit(cbowHiddenLayerWeights)\n",
    "analogyResultsSkipgram1 = makeAnalogyOnFile(nbrs, EMBEDDING_TYPE_2, cbowHiddenLayerWeights, full_index)\n",
    "print(\"Score for cbow (extra hideen layer) embedding: %s\" % sum(analogyResultsSkipgram1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "############ 4c. Visualize your results and interpret your results #############\n",
    "################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing words in the index \n",
      "\tComparing usual vs. usually (word vs. prediction)\n",
      "'sudden suddenly usual usually': False\n",
      "\tComparing good vs. bad (word vs. prediction)\n",
      "'bad worse good better': False\n",
      "\tComparing look vs. looking (word vs. prediction)\n",
      "'go going look looking': False\n",
      "\tComparing his vs. particular (word vs. prediction)\n",
      "'he she his her': False\n",
      "\tComparing his vs. her (word vs. prediction)\n",
      "'brother sister his her': False\n",
      "\tComparing look vs. looking (word vs. prediction)\n",
      "'listen listening look looking': False\n",
      "\tComparing thinking vs. continued (word vs. prediction)\n",
      "'saying said thinking thought': False\n",
      "\tComparing cat vs. cats (word vs. prediction)\n",
      "'bird birds cat cats': False\n",
      "\tComparing old vs. good (word vs. prediction)\n",
      "'good better old older': False\n",
      "\tComparing quick vs. good (word vs. prediction)\n",
      "'good better quick quicker': False\n",
      "\tComparing good vs. large (word vs. prediction)\n",
      "'large largest good best': False\n",
      "Missing words in the index \n",
      "\tComparing knowing vs. knew (word vs. prediction)\n",
      "'falling fell knowing knew': False\n",
      "\tComparing think vs. walk (word vs. prediction)\n",
      "'walk walking think thinking': False\n",
      "\tComparing cat vs. cats (word vs. prediction)\n",
      "'child children cat cats': False\n",
      "\tComparing eye vs. eyes (word vs. prediction)\n",
      "'dog dogs eye eyes': False\n",
      "\tComparing rat vs. hand (word vs. prediction)\n",
      "'hand hands rat rats': False\n",
      "\tComparing find vs. eat (word vs. prediction)\n",
      "'eat eats find finds': False\n",
      "\tComparing say vs. find (word vs. prediction)\n",
      "'find finds say says': False\n",
      "\tComparing good vs. old (word vs. prediction)\n",
      "'old older good better': False\n",
      "\tComparing quick vs. large (word vs. prediction)\n",
      "'large larger quick quicker': False\n",
      "\tComparing listen vs. listening (word vs. prediction)\n",
      "'go going listen listening': False\n",
      "\tComparing walk vs. walking (word vs. prediction)\n",
      "'run running walk walking': False\n",
      "\tComparing think vs. run (word vs. prediction)\n",
      "'run running think thinking': False\n",
      "Missing words in the index \n",
      "\tComparing sit vs. say (word vs. prediction)\n",
      "'say saying sit sitting': False\n",
      "Missing words in the index \n",
      "\tComparing rabbit vs. temper (word vs. prediction)\n",
      "'alice she rabbit he': False\n",
      "\tComparing rabbit vs. duck (word vs. prediction)\n",
      "'alice her rabbit him': False\n",
      "\tComparing rabbit vs. alice (word vs. prediction)\n",
      "'alice girl rabbit sir': False\n",
      "Missing words in the index \n",
      "Missing words in the index \n",
      "\tComparing alice vs. dinah (word vs. prediction)\n",
      "'dinah cat alice girl': False\n",
      "Missing words in the index \n",
      "\tComparing he vs. she (word vs. prediction)\n",
      "'his her he she': False\n",
      "\tComparing quick vs. long (word vs. prediction)\n",
      "'long longer quick quicker': False\n",
      "\tComparing small vs. long (word vs. prediction)\n",
      "'long longer small smaller': False\n",
      "\tComparing bad vs. long (word vs. prediction)\n",
      "'long longer bad worse': False\n",
      "\tComparing look vs. looking (word vs. prediction)\n",
      "'go going look looking': False\n",
      "\tComparing look vs. looking (word vs. prediction)\n",
      "'listen listening look looking': False\n",
      "\tComparing sit vs. sitting (word vs. prediction)\n",
      "'swim swimming sit sitting': False\n",
      "\tComparing listen vs. listening (word vs. prediction)\n",
      "'run running listen listening': False\n",
      "\tComparing read vs. think (word vs. prediction)\n",
      "'think thinking read reading': False\n",
      "\tComparing close vs. eaten (word vs. prediction)\n",
      "'up down close far': False\n",
      "Missing words in the index \n",
      "Missing words in the index \n",
      "Score for w-w co-ocurrence matrix embedding: 0\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "############# 5. Use the word co-occurence matrix from Question 1 ##############\n",
    "############# Compare the performance on the analogy task.        ##############\n",
    "################################################################################\n",
    "    \n",
    "nbrs = NearestNeighbors(n_neighbors = 1).fit(wcoMatrix)\n",
    "analogyResultsWcoMatrix = makeAnalogyOnFile(nbrs, EMBEDDING_TYPE_1, wcoMatrix, full_index)\n",
    "print(\"Score for w-w co-ocurrence matrix embedding: %s\" % sum(analogyResultsWcoMatrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "############## 7b. Performance on the analogy task Glove-word2vec ##############\n",
    "################################################################################\n",
    "\n",
    "# Get labels \n",
    "sorted_labels = sorted(full_index.items(), key=operator.itemgetter(1))\n",
    "sorted_labels.insert(0, ('', 0))\n",
    "labels_only = [i[0] for i in sorted_labels]\n",
    "labels = np.asarray(labels_only)\n",
    "\n",
    "##################################### CBOW #####################################\n",
    "\n",
    "# Transform data to 2d\n",
    "tsne = TSNE(n_components=2, verbose=1)\n",
    "transformed_weights = tsne.fit_transform(cbowWeights)\n",
    "\n",
    "# Visualize\n",
    "plot_embedding(transformed_weights, labels, \"Embedding for simple CBOW\")\n",
    "\n",
    "\n",
    "################################### Skipgram ###################################\n",
    "\n",
    "# Transform data to 2d\n",
    "tsne = TSNE(n_components=2, verbose=1)\n",
    "transformed_weights = tsne.fit_transform(skipgramWeights)\n",
    "\n",
    "# Visualize\n",
    "plot_embedding(transformed_weights, labels, \"Embedding for simple Skipgram\")\n",
    "\n",
    "\n",
    "############################## CBOW (extra layer) ##############################\n",
    "\n",
    "# Transform data to 2d\n",
    "tsne = TSNE(n_components=2, verbose=1)\n",
    "transformed_weights = tsne.fit_transform(cbowHiddenLayerWeights)\n",
    "\n",
    "# Visualize\n",
    "plot_embedding(transformed_weights, labels, \"Embedding for CBOW (extra layer)\")\n",
    "\n",
    "\n",
    "############################ Skipgram (extra layer) ############################\n",
    "\n",
    "# Transform data to 2d\n",
    "tsne = TSNE(n_components=2, verbose=1)\n",
    "transformed_weights = tsne.fit_transform(skipgramHiddenLayerWeights)\n",
    "\n",
    "# Visualize\n",
    "plot_embedding(transformed_weights, labels, \"Embedding for Skipgram (extra layer)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "############# 5. Use the word co-occurence matrix from Question 1. #############\n",
    "################################################################################\n",
    "\n",
    "# Compare the performance on the analogy task with the performance of your trained word embeddings.  \n",
    "analogyResultsWcoMatrix = makeAnalogyOnFile()\n",
    "\n",
    "print(\"\\nScore for w-w co-ocurrence matrix embedding: %s\" % sum(analogyResultsWcoMatrix))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation results of the visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results of the trained word embeddings with the word-word co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Discuss**\n",
    "\n",
    "Jorge > We can pick ideas from the paper and the book. Should be easy\n",
    "\n",
    "#### What are the main advantages of CBOW and Skipgram?\n",
    "\n",
    "\n",
    "#### What is the advantage of negative sampling?\n",
    "\n",
    "\n",
    "#### What are the main drawbacks of CBOW and Skipgram?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Load pre-trained embeddings on large corpuses (see the pdf file). You only have to consider the word embeddings with an embedding size of 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "############## 7a. Load pre-trained embeddings on large corpuses  ##############\n",
    "################################################################################\n",
    "\n",
    "#load pretrained word embeddings of word2vec\n",
    "path_word2vec = \"./GoogleNews-vectors-negative300.bin\"\n",
    "word2vec = KeyedVectors.load_word2vec_format(path_word2vec, binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pretraind word embeddings of Glove\n",
    "\n",
    "#path = \"./glove.6B/glove.6B.300d_converted.txt\"\n",
    "path = \"./glove.6B/glove.6B.300d.txt\"\n",
    "path2 = \"./glove_converted.txt\"\n",
    "\n",
    "#convert GloVe into word2vec format\n",
    "gensim.scripts.glove2word2vec.get_glove_info(path)\n",
    "gensim.scripts.glove2word2vec.glove2word2vec(path, \"glove_converted.txt\")\n",
    "\n",
    "glove = KeyedVectors.load_word2vec_format(path2, binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tComparing pleasant vs. unpleasant (word vs. prediction)\n",
      "'happy unhappy pleasant unpleasant': False\n",
      "\tComparing usual vs. usually (word vs. prediction)\n",
      "'sudden suddenly usual usually': False\n",
      "\tComparing good vs. good (word vs. prediction)\n",
      "'bad worse good better': True\n",
      "\tComparing look vs. looking (word vs. prediction)\n",
      "'go going look looking': False\n",
      "\tComparing his vs. his (word vs. prediction)\n",
      "'he she his her': True\n",
      "\tComparing his vs. his (word vs. prediction)\n",
      "'brother sister his her': True\n",
      "\tComparing look vs. looking (word vs. prediction)\n",
      "'listen listening look looking': False\n",
      "\tComparing thinking vs. thought (word vs. prediction)\n",
      "'saying said thinking thought': False\n",
      "\tComparing cat vs. cat (word vs. prediction)\n",
      "'bird birds cat cats': True\n",
      "\tComparing old vs. older (word vs. prediction)\n",
      "'good better old older': False\n",
      "\tComparing quick vs. quicker (word vs. prediction)\n",
      "'good better quick quicker': False\n",
      "\tComparing good vs. good (word vs. prediction)\n",
      "'large largest good best': True\n",
      "\tComparing comfortable vs. uncomfortable (word vs. prediction)\n",
      "'happy unhappy comfortable uncomfortable': False\n",
      "\tComparing knowing vs. knew (word vs. prediction)\n",
      "'falling fell knowing knew': False\n",
      "\tComparing think vs. thinking (word vs. prediction)\n",
      "'walk walking think thinking': False\n",
      "\tComparing cat vs. cats (word vs. prediction)\n",
      "'child children cat cats': False\n",
      "\tComparing eye vs. eyes (word vs. prediction)\n",
      "'dog dogs eye eyes': False\n",
      "\tComparing rat vs. rats (word vs. prediction)\n",
      "'hand hands rat rats': False\n",
      "\tComparing find vs. finds (word vs. prediction)\n",
      "'eat eats find finds': False\n",
      "\tComparing say vs. said (word vs. prediction)\n",
      "'find finds say says': False\n",
      "\tComparing good vs. better (word vs. prediction)\n",
      "'old older good better': False\n",
      "\tComparing quick vs. quicker (word vs. prediction)\n",
      "'large larger quick quicker': False\n",
      "\tComparing listen vs. listening (word vs. prediction)\n",
      "'go going listen listening': False\n",
      "\tComparing walk vs. walking (word vs. prediction)\n",
      "'run running walk walking': False\n",
      "\tComparing think vs. thinking (word vs. prediction)\n",
      "'run running think thinking': False\n",
      "\tComparing large vs. larger (word vs. prediction)\n",
      "'pleasant pleasanter large larger': False\n",
      "\tComparing sit vs. sitting (word vs. prediction)\n",
      "'say saying sit sitting': False\n",
      "\tComparing happy vs. unhappy (word vs. prediction)\n",
      "'wrong true happy unhappy': False\n",
      "\tComparing rabbit vs. alice (word vs. prediction)\n",
      "'alice she rabbit he': False\n",
      "\tComparing rabbit vs. alice (word vs. prediction)\n",
      "'alice her rabbit him': False\n",
      "\tComparing rabbit vs. sir (word vs. prediction)\n",
      "'alice girl rabbit sir': False\n",
      "\tComparing sudden vs. uneasily (word vs. prediction)\n",
      "'uneasily easily sudden suddenly': False\n",
      "\tComparing calmly vs. uneasily (word vs. prediction)\n",
      "'uneasily easily calmly angrily': False\n",
      "Missing words in the index \n",
      "\tComparing happy vs. unhappy (word vs. prediction)\n",
      "'never always happy unhappy': False\n",
      "\tComparing he vs. he (word vs. prediction)\n",
      "'his her he she': True\n",
      "\tComparing quick vs. quicker (word vs. prediction)\n",
      "'long longer quick quicker': False\n",
      "\tComparing small vs. smaller (word vs. prediction)\n",
      "'long longer small smaller': False\n",
      "\tComparing bad vs. worse (word vs. prediction)\n",
      "'long longer bad worse': False\n",
      "\tComparing look vs. looking (word vs. prediction)\n",
      "'go going look looking': False\n",
      "\tComparing look vs. looking (word vs. prediction)\n",
      "'listen listening look looking': False\n",
      "\tComparing sit vs. sitting (word vs. prediction)\n",
      "'swim swimming sit sitting': False\n",
      "\tComparing listen vs. listening (word vs. prediction)\n",
      "'run running listen listening': False\n",
      "\tComparing read vs. reading (word vs. prediction)\n",
      "'think thinking read reading': False\n",
      "\tComparing close vs. far (word vs. prediction)\n",
      "'up down close far': False\n",
      "\tComparing king vs. queen (word vs. prediction)\n",
      "'queen woman king man': False\n",
      "\tComparing man vs. man (word vs. prediction)\n",
      "'boy girl man woman': True\n",
      "Score for GoogleNews embedding: 7\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "############# 7b. Performance on the analogy task Google-word2vec  #############\n",
    "################################################################################\n",
    "\n",
    "analogyResultsWord2Vec = makeAnalogyOnFile(neighbors = None, embeddingType = EMBEDDING_TYPE_4, matrix = None, index = word2vec)\n",
    "print(\"Score for GoogleNews embedding: %s\" % sum(analogyResultsWord2Vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tComparing pleasant vs. unpleasant (word vs. prediction)\n",
      "'happy unhappy pleasant unpleasant': False\n",
      "\tComparing usual vs. usually (word vs. prediction)\n",
      "'sudden suddenly usual usually': False\n",
      "\tComparing good vs. good (word vs. prediction)\n",
      "'bad worse good better': True\n",
      "\tComparing look vs. looking (word vs. prediction)\n",
      "'go going look looking': False\n",
      "\tComparing his vs. his (word vs. prediction)\n",
      "'he she his her': True\n",
      "\tComparing his vs. his (word vs. prediction)\n",
      "'brother sister his her': True\n",
      "\tComparing look vs. looking (word vs. prediction)\n",
      "'listen listening look looking': False\n",
      "\tComparing thinking vs. thought (word vs. prediction)\n",
      "'saying said thinking thought': False\n",
      "\tComparing cat vs. cats (word vs. prediction)\n",
      "'bird birds cat cats': False\n",
      "\tComparing old vs. older (word vs. prediction)\n",
      "'good better old older': False\n",
      "\tComparing quick vs. quicker (word vs. prediction)\n",
      "'good better quick quicker': False\n",
      "\tComparing good vs. best (word vs. prediction)\n",
      "'large largest good best': False\n",
      "\tComparing comfortable vs. uncomfortable (word vs. prediction)\n",
      "'happy unhappy comfortable uncomfortable': False\n",
      "\tComparing knowing vs. knew (word vs. prediction)\n",
      "'falling fell knowing knew': False\n",
      "\tComparing think vs. thinking (word vs. prediction)\n",
      "'walk walking think thinking': False\n",
      "\tComparing cat vs. cats (word vs. prediction)\n",
      "'child children cat cats': False\n",
      "\tComparing eye vs. eyes (word vs. prediction)\n",
      "'dog dogs eye eyes': False\n",
      "\tComparing rat vs. rats (word vs. prediction)\n",
      "'hand hands rat rats': False\n",
      "\tComparing find vs. find (word vs. prediction)\n",
      "'eat eats find finds': True\n",
      "\tComparing say vs. says (word vs. prediction)\n",
      "'find finds say says': False\n",
      "\tComparing good vs. old (word vs. prediction)\n",
      "'old older good better': False\n",
      "\tComparing quick vs. quicker (word vs. prediction)\n",
      "'large larger quick quicker': False\n",
      "\tComparing listen vs. listening (word vs. prediction)\n",
      "'go going listen listening': False\n",
      "\tComparing walk vs. walking (word vs. prediction)\n",
      "'run running walk walking': False\n",
      "\tComparing think vs. thinking (word vs. prediction)\n",
      "'run running think thinking': False\n",
      "Missing words in the index \n",
      "\tComparing sit vs. sitting (word vs. prediction)\n",
      "'say saying sit sitting': False\n",
      "\tComparing happy vs. unhappy (word vs. prediction)\n",
      "'wrong true happy unhappy': False\n",
      "\tComparing rabbit vs. alice (word vs. prediction)\n",
      "'alice she rabbit he': False\n",
      "\tComparing rabbit vs. alice (word vs. prediction)\n",
      "'alice her rabbit him': False\n",
      "\tComparing rabbit vs. sir (word vs. prediction)\n",
      "'alice girl rabbit sir': False\n",
      "\tComparing sudden vs. uneasily (word vs. prediction)\n",
      "'uneasily easily sudden suddenly': False\n",
      "\tComparing calmly vs. uneasily (word vs. prediction)\n",
      "'uneasily easily calmly angrily': False\n",
      "\tComparing alice vs. dinah (word vs. prediction)\n",
      "'dinah cat alice girl': False\n",
      "\tComparing happy vs. unhappy (word vs. prediction)\n",
      "'never always happy unhappy': False\n",
      "\tComparing he vs. he (word vs. prediction)\n",
      "'his her he she': True\n",
      "\tComparing quick vs. quicker (word vs. prediction)\n",
      "'long longer quick quicker': False\n",
      "\tComparing small vs. smaller (word vs. prediction)\n",
      "'long longer small smaller': False\n",
      "\tComparing bad vs. worse (word vs. prediction)\n",
      "'long longer bad worse': False\n",
      "\tComparing look vs. looking (word vs. prediction)\n",
      "'go going look looking': False\n",
      "\tComparing look vs. looking (word vs. prediction)\n",
      "'listen listening look looking': False\n",
      "\tComparing sit vs. sitting (word vs. prediction)\n",
      "'swim swimming sit sitting': False\n",
      "\tComparing listen vs. listening (word vs. prediction)\n",
      "'run running listen listening': False\n",
      "\tComparing read vs. reading (word vs. prediction)\n",
      "'think thinking read reading': False\n",
      "\tComparing close vs. far (word vs. prediction)\n",
      "'up down close far': False\n",
      "\tComparing king vs. queen (word vs. prediction)\n",
      "'queen woman king man': False\n",
      "\tComparing man vs. woman (word vs. prediction)\n",
      "'boy girl man woman': False\n",
      "Score for Glove embedding: 7\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "############## 7b. Performance on the analogy task Glove-word2vec ##############\n",
    "################################################################################\n",
    "\n",
    "analogyResultsGlove = makeAnalogyOnFile(neighbors = None, embeddingType = EMBEDDING_TYPE_4, matrix = None, index = glove)\n",
    "print(\"Score for Glove embedding: %s\" % sum(analogyResultsWord2Vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "###################### 7. Visualize embeddings (word2vec) ######################\n",
    "################################################################################\n",
    "\n",
    "################################## GoogleNews ##################################\n",
    "\n",
    "#Transform data to 2d\n",
    "tsne = TSNE(n_components=2, verbose=1)\n",
    "transformed_weights = tsne.fit_transform(word2vec.vectors)\n",
    "\n",
    "# Get labels \n",
    "sorted_labels = sorted(full_index.items(), key=operator.itemgetter(1))\n",
    "sorted_labels.insert(0, ('', 0))\n",
    "labels_only = [i[0] for i in sorted_labels]\n",
    "labels = np.asarray(labels_only)\n",
    "\n",
    "# Visualize\n",
    "plot_embedding(transformed_weights, labels, \"Embedding for word2vec (Google)\")\n",
    "\n",
    "#################################### Glove ####################################\n",
    "\n",
    "#Transform data to 2d\n",
    "tsne = TSNE(n_components=2, verbose=1)\n",
    "transformed_weights = tsne.fit_transform(glove.vectors)\n",
    "\n",
    "# Get labels \n",
    "sorted_labels = sorted(full_index.items(), key=operator.itemgetter(1))\n",
    "sorted_labels.insert(0, ('', 0))\n",
    "labels_only = [i[0] for i in sorted_labels]\n",
    "labels = np.asarray(labels_only)\n",
    "\n",
    "# Visualize\n",
    "plot_embedding(transformed_weights, labels, \"Embedding for word2vec (Google)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison performance with your own trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
