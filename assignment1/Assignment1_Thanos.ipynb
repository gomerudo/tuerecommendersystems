{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "**Group 22**\n",
    "\n",
    "A. Siganos ( 1283871 )     \n",
    "J. Gómez Robles ( 1286552 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20175484\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\20175484\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(13) #TODO Check if this is used for sgd\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Reshape, Lambda\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.preprocessing import sequence\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors as nn\n",
    "from matplotlib import pylab\n",
    "from __future__ import division\n",
    "\n",
    "import time\n",
    "# Only for the word2vec parsing of last exercise (it was part of the template, not our implementation)\n",
    "#import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT Modify the lines in this cell\n",
    "path = 'alice.txt'\n",
    "corpus = open(path).readlines()[0:700]\n",
    "\n",
    "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
    "\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"'\")\n",
    "tokenizer.fit_on_texts(corpus) #list of texts to train on\n",
    "\n",
    "corpus = tokenizer.texts_to_sequences(corpus)  #list of texts to turn to sequences, Basically words->indices\n",
    "nb_samples = sum(len(s) for s in corpus)       # number of words in corpus\n",
    "V = len(tokenizer.word_index) + 1              #dictionary mapping words (str) to their rank/index(int). number of words in dictionary(unique words)\n",
    "                                             #Only set after fit_on_texts was called.\n",
    "# Is this something they need to change?\n",
    "dim = 100\n",
    "window_size = 2 #use this window size for Skipgram, CBOW, and the model with the additional hidden layer\n",
    "window_size_corpus = 4 #use this window size for the co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "### Co-occurrence Matrix\n",
    "Use the provided code to load the \"Alice in Wonderland\" text document. \n",
    "1. Implement the word-word co-occurrence matrix for “Alice in Wonderland”\n",
    "2. Normalize the words such that every value lies within a range of 0 and 1\n",
    "3. Compute the cosine distance between the given words:\n",
    "    - Alice \n",
    "    - Dinah\n",
    "    - Rabbit\n",
    "4. List the 5 closest words to 'Alice'. Discuss the results.\n",
    "5. Discuss what the main drawbacks are of a term-term co-occurence matrix solutions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "############################## 0. Util functions ###############################\n",
    "################################################################################\n",
    "\n",
    "# Run this block first\n",
    "def getKeyFromValue(dictionary, target):\n",
    "    return list(dictionary.keys())[list(dictionary.values()).index(target)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sumary of the word-word co-ocurrence matrix\n",
      " - Shape (1183, 1183)\n",
      " - Min value 0.0\n",
      " - Max value 1.0\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "################# 1. Implement word-word co-ocurrence matrix ###################\n",
    "################################################################################\n",
    "\n",
    "# Create my co-ocurrence matrix, initially 0 (VxV size)\n",
    "# How to access this matrix:\n",
    "#   Each column is an (index - 1) (column 0 is word in index 1; column 1 is word in index 2; ...)\n",
    "wcoMatrix = np.zeros([V, V]) \n",
    "\n",
    "# Using my window_size_corpus to define my context scope\n",
    "scope = window_size_corpus\n",
    "\n",
    "# TODO: Do not make that many iterations\n",
    "# Greedy approach first (to be able to compare the optimization)\n",
    "\n",
    "# For each line in the corpus. Note that they preserve the order, even when they are indexes now.\n",
    "for s in corpus:\n",
    "    # For each word\n",
    "    for current_index in range(0, len(s)):\n",
    "        current_value = s[current_index] # Get the 'word'\n",
    "        # From left to right\n",
    "        for neighbor_index in range(current_index - scope, current_index + scope + 1):\n",
    "            # Never out of boundaries and never the same index\n",
    "            if ( neighbor_index >= 0 and neighbor_index < len(s) ) and ( neighbor_index != current_index ):\n",
    "                # Get my neighbor 'word'\n",
    "                neighbor = s[neighbor_index] \n",
    "                # Never myself and myself (keeping diagonal to 0)\n",
    "                if current_value != neighbor:\n",
    "                    # Update the ocurrence\n",
    "                    #wcoMatrix[current_value - 1, neighbor - 1] += 1\n",
    "                    wcoMatrix[current_value, neighbor] += 1\n",
    "                    \n",
    "################################################################################\n",
    "################################# 2. Normalize #################################\n",
    "################################################################################\n",
    "wcoMatrix = wcoMatrix / wcoMatrix.max()\n",
    "wcoMatrix\n",
    "\n",
    "print(\"Sumary of the word-word co-ocurrence matrix\")\n",
    "print(\" - Shape\", wcoMatrix.shape)\n",
    "print(\" - Min value\", wcoMatrix.min())\n",
    "print(\" - Max value\", wcoMatrix.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity(alice, dinah)=[[0.39360011]]\n",
      "cosine_similarity(alice, rabbit)=[[0.47890931]]\n",
      "cosine_similarity(dinah, alice)=[[0.39360011]]\n",
      "cosine_similarity(dinah, rabbit)=[[0.29862324]]\n",
      "cosine_similarity(rabbit, alice)=[[0.47890931]]\n",
      "cosine_similarity(rabbit, dinah)=[[0.29862324]]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "########### 3. Compute cosine similarity to Alice, Dinah and Rabbit ############\n",
    "################################################################################\n",
    "\n",
    "words_to_compare = [\"Alice\", \"Dinah\", \"Rabbit\"]\n",
    "\n",
    "# Get my index\n",
    "full_index = tokenizer.word_index\n",
    "\n",
    "# Iterate on all possible combinations\n",
    "for w1 in words_to_compare:\n",
    "    w1 = w1.lower() \n",
    "    x = full_index[w1]\n",
    "    for w2 in words_to_compare:\n",
    "        w2 = w2.lower()\n",
    "        if w1 != w2:\n",
    "            y = full_index[w2]\n",
    "            X = wcoMatrix[x, :].reshape((1, V))\n",
    "            Y = wcoMatrix[y, :].reshape((1, V))\n",
    "            print(\"cosine_similarity(%s, %s)=%s\" % (w1, w2, cosine_similarity(X, Y)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors (co-ocurrence matrix based) to alice (index 11):\n",
      "her [index 14] with distance: 0.5616433150108668\n",
      "that [index 13] with distance: 0.5692975698101006\n",
      "herself [index 41] with distance: 0.5700573426956416\n",
      "for [index 18] with distance: 0.5811858638252018\n",
      "on [index 21] with distance: 0.5837124477057559\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "################## 4a. List the 5 closest words to 'Alice'. ####################\n",
    "################################################################################\n",
    "\n",
    "################################\n",
    "### We use Nearest Neighbors ###\n",
    "################################\n",
    "\n",
    "# It was not importing\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Define our variables\n",
    "targetWord = \"Alice\".lower()\n",
    "X = wcoMatrix\n",
    "N_NEIGHBORS = 5\n",
    "full_index = tokenizer.word_index\n",
    "\n",
    "# Perfrom the NearestNeibhbors algorithm\n",
    "# TODO: Tune parameters\n",
    "nbrs = NearestNeighbors(n_neighbors = N_NEIGHBORS).fit(X)\n",
    "\n",
    "# Get the index corresponding to the target word\n",
    "aliceMatrixIndex = full_index[targetWord]\n",
    "\n",
    "# Get results from nearest neighbors\n",
    "distances, indices = nbrs.kneighbors()\n",
    "aliceIndices = indices[aliceMatrixIndex, :]\n",
    "aliceDistances = distances[aliceMatrixIndex, :]\n",
    "\n",
    "# Output result\n",
    "print(\"Nearest neighbors (co-ocurrence matrix based) to %s (index %s):\" % (targetWord, aliceMatrixIndex))\n",
    "for i in range(0, N_NEIGHBORS):\n",
    "    print( \"%s [index %s] with distance: %s\" % (getKeyFromValue(full_index, aliceIndices[i]), aliceIndices[i], aliceDistances[i]) )\n",
    "\n",
    "# TODO: Think about what we are computing. \n",
    "# With first approach, a neighbor would be a word with a similar vector in the co-ocurrence matrix, \n",
    "# not a word within the same context (window_size). Although words in the similar window_size should have a\n",
    "# similar vector, isn't?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4b. Discuss the results**\n",
    "\n",
    "We observe that the most similar words are possesives and prepositions. Although these results intuitively make some sense (Alice is the main character of the book and consequently, describing her behaviour is expected), no idiomatic relations can be achieved by means of the simple word-word co-ocurrence matrix.\n",
    "\n",
    "Furthermore, the minimal distance (cosine similarity normalized) found for alice is of 0.5616, which suggests a poor result - one would be interested on a similarity close to 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Discuss what the main drawbacks are of a term-term co-occurence matrix solutions? **\n",
    "\n",
    "TODO. Add some ideas!\n",
    "\n",
    "A co-occurence matrix  requires large memory for storage especially for larger data sets. A co-occurence matrix also has the negative of losing the \"polysemantic\" words. For example if inside our corpus there is such a word, lets say \"jaguar\" with both meanings, that is the car brand and the animal. When we translate the corpus in a one hote encoding dictionary then we actually lose that information.\n",
    "\n",
    "Jorge's ideas: sparse matrix VxV size (computation time increases).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save your all the vector representations of your word embeddings in this way\n",
    "#Change when necessary the sizes of the vocabulary/embedding dimension\n",
    "\n",
    "f = open('vectors_co_occurrence.txt',\"w\")\n",
    "f.write(\" \".join([str(V-1),str(V-1)]))\n",
    "f.write(\"\\n\")\n",
    "\n",
    "#vectors = your word co-occurrence matrix\n",
    "vectors = wcoMatrix\n",
    "for word, i in tokenizer.word_index.items():    \n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reopen your file as follows\n",
    "# TODO: Check! It is not reloading the file D:\n",
    "# co_occurrence = KeyedVectors.load_word2vec_format('./vectors_co_occurrence.txt', binary=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "### Word embeddings\n",
    "Build embeddings with a keras implementation where the embedding vector is of length 50, 150 and 300. Use the Alice in Wonderland text book for training.\n",
    "1. Using the CBOW model\n",
    "2. Using Skipgram model\n",
    "3. Add extra hidden dense layer to CBow and Skipgram implementations. Choose an activation function for that layer and justify your answer.\n",
    "4. Analyze the four different word embeddings\n",
    "    - Implement your own function to perform the analogy task with. Do not use existing libraries for this task such as Gensim. Your function should be able to answer whether an anaology as in the example given in the pdf-file is true.\n",
    "    - Compare the performance on the analogy task between the word embeddings that you have trained in 2.1, 2.2 and 2.3.  \n",
    "    - Visualize your results and interpret your results\n",
    "5. Use the word co-occurence matrix from Question 1. Compare the performance on the analogy task with the performance of your trained word embeddings.  \n",
    "6. Discuss:\n",
    "    - What are the main advantages of CBOW and Skipgram?\n",
    "    - What is the advantage of negative sampling?\n",
    "    - What are the main drawbacks of CBOW and Skipgram?\n",
    "7. Load pre-trained embeddings on large corpuses (see the pdf file). You only have to consider the word embeddings with an embedding size of 300\n",
    "    - Compare performance on the analogy task with your own trained embeddings from \"Alice in Wonderland\". You can limit yourself to the vocabulary of Alice in Wonderland. Visualize the pre-trained word embeddings and compare these with the results of your own trained word embeddings. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare data for cbow\n",
    "def generate_data_cbow(corpus, window_size, V):\n",
    "    all_in = [] \n",
    "    all_out = []\n",
    "    maxlen = window_size*2\n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            contexts = []\n",
    "            labels   = []            \n",
    "            s = index - window_size\n",
    "            e = index + window_size + 1\n",
    "            \n",
    "            contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
    "            labels.append(word)\n",
    "            if contexts != []:\n",
    "                all_in.append(sequence.pad_sequences(contexts, maxlen = maxlen))\n",
    "                all_out.append(np_utils.to_categorical(labels, V))\n",
    "    return (all_in,all_out) # return the values\n",
    "\n",
    "            \n",
    "            \n",
    "# def generate_data_skipgram(corpus, window_size, V):\n",
    "#     all_in = [] \n",
    "#     all_out = []\n",
    "#     for sentence in corpus: # For, sentence in corpus\n",
    "#         L = len(sentence) # Limit for the iteration (per sentence)\n",
    "\n",
    "#         # For each pair in the index\n",
    "#         for index, word in enumerate(sentence):\n",
    "#             # Create X and Y\n",
    "#             in_words = [] # list of lists\n",
    "#             labels = [] # list of integers\n",
    "\n",
    "#             # Iterate over all the sentence, for each word in the line. I.e, for each word:\n",
    "#             for i in range(index - window_size, index + window_size + 1):\n",
    "#                 # If between the limits and not myself\n",
    "#                 if i != index and (0 <= i < L):\n",
    "#                     # This means: Associating the words in my neighbor (context) \n",
    "#                     in_words.append([word]) # Current word (as a list) is associated to...\n",
    "#                     labels.append(sentence[i]) # ... this word (as a simple integer).\n",
    "                    \n",
    "#             # Once I have saved the context for the current word...\n",
    "#             # ... if there is something in the arrays\n",
    "#             if in_words != []:\n",
    "#                 # Save the information to the all_in/out arrays\n",
    "#                 all_in.append(np.array(in_words,dtype=np.int32)) # X in integer format\n",
    "#                 # Set Y to categorical values. If my output is ([6, 26]). Then I create\n",
    "#                 # a vector from 0 to V and mark 6 and 26 as 1\n",
    "#                 all_out.append(np_utils.to_categorical(labels, V))\n",
    "#     return (all_in,all_out) # return the values\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create CBOW model\n",
    "\n",
    "def getCbowModel(dim = 50, extra_layer = False, activation_extra_layer = 'softmax'):\n",
    "\n",
    "    # The Sequential model is a linear stack of layers.\n",
    "    cbow = Sequential()\n",
    "    cbow.add(Embedding(input_dim = V, output_dim = dim, input_length = window_size*2))\n",
    "\n",
    "    # TODO!!! \n",
    "    if extra_layer :\n",
    "        cbow.add(Dense(input_dim = V, units=dim, kernel_initializer='uniform', activation = activation_extra_layer))\n",
    "        # input_dim=V input ,units=dim output\n",
    "    cbow.add( Lambda(lambda x: K.mean(x, axis = 1), output_shape = (dim,)) )\n",
    "    cbow.add( Dense(V, activation = 'softmax') )\n",
    "\n",
    "    # define loss function for Cbow\n",
    "    cbow.compile(loss='categorical_crossentropy', optimizer = 'adadelta')\n",
    "\n",
    "    return cbow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running CBOW (dim = 50)...\n",
      "0 60195.72382962704\n",
      "1 53692.501888513565\n",
      "2 52610.73432683945\n",
      "3 52110.761258006096\n",
      "4 51726.76455152035\n",
      "5 51412.44442987442\n",
      "6 51144.94545054436\n",
      "7 50911.15686917305\n",
      "8 50702.69039058685\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-98b15974bc8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcbow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, class_weight, sample_weight)\u001b[0m\n\u001b[0;32m   1067\u001b[0m         return self.model.train_on_batch(x, y,\n\u001b[0;32m   1068\u001b[0m                                          \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1069\u001b[1;33m                                          class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1070\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1071\u001b[0m     def test_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1847\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1848\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1849\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1851\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2475\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2476\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train model\n",
    "\n",
    "################################################################################\n",
    "############################ 1. Using CBOW model ###############################\n",
    "################################################################################\n",
    "\n",
    "dims = [50, 150, 300]\n",
    "iterations = 10\n",
    "listResX = []\n",
    "listResY = []\n",
    "\n",
    "for dimension in dims:\n",
    "    print(\"Running CBOW (dim = %s)...\"% dimension)\n",
    "    cbow = getCbowModel(dimension, False)\n",
    "    X, Y = generate_data_cbow(corpus, window_size, V)\n",
    "    start_time=time.time()\n",
    "    for ite in range(10):\n",
    "        loss = 0.\n",
    "        for x, y in zip(X, Y):\n",
    "            loss += cbow.train_on_batch(x, y)\n",
    "        print(ite, loss)\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(\"--- %s seconds ---\" % (end_time - start_time))\n",
    "    listResX.append(X)\n",
    "    listResY.append(Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare data for Skipgram\n",
    "\n",
    "# This function creates a list where _each word_ in the text file (with non-empty content)\n",
    "# is encoded into two arrays:\n",
    "#   - x: an array of the size of all his windows_size neighbors with the value of the word (i.e, repeated values)\n",
    "#   - y: an array of the same size as x pointing to each neighbor. Finally encoded in a |V|-list as one-hot enc\n",
    "def generate_data_skipgram(corpus, window_size, V):\n",
    "    all_in = [] \n",
    "    all_out = []\n",
    "    for sentence in corpus: # For, sentence in corpus\n",
    "        L = len(sentence) # Limit for the iteration (per sentence)\n",
    "\n",
    "        # For each pair in the index\n",
    "        for index, word in enumerate(sentence):\n",
    "            # Create X and Y\n",
    "            in_words = [] # list of lists\n",
    "            labels = [] # list of integers\n",
    "\n",
    "            # Iterate over all the sentence, for each word in the line. I.e, for each word:\n",
    "            for i in range(index - window_size, index + window_size + 1):\n",
    "                # If between the limits and not myself\n",
    "                if i != index and (0 <= i < L):\n",
    "                    # This means: Associating the words in my neighbor (context) \n",
    "                    in_words.append([word]) # Current word (as a list) is associated to...\n",
    "                    labels.append(sentence[i]) # ... this word (as a simple integer).\n",
    "                    \n",
    "            # Once I have saved the context for the current word...\n",
    "            # ... if there is something in the arrays\n",
    "            if in_words != []:\n",
    "                # Save the information to the all_in/out arrays\n",
    "                all_in.append(np.array(in_words,dtype=np.int32)) # X in integer format\n",
    "                # Set Y to categorical values. If my output is ([6, 26]). Then I create\n",
    "                # a vector from 0 to V and mark 6 and 26 as 1\n",
    "                all_out.append(np_utils.to_categorical(labels, V))\n",
    "    return (all_in,all_out) # return the values\n",
    "\n",
    "# Prepare data for skipgram\n",
    "#X, Y = generate_data_skipgram(corpus, window_size, V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #save the preprocessed data of Skipgram\n",
    "# f = open('data_skipgram.txt' ,'w')\n",
    "\n",
    "# for input, outcome in zip(X,Y):\n",
    "#     input = np.concatenate(input)\n",
    "#     f.write(\" \".join(map(str, list(input))))\n",
    "#     f.write(\",\")\n",
    "#     outcome = np.concatenate(outcome)\n",
    "#     f.write(\" \".join(map(str,list(outcome))))\n",
    "#     f.write(\"\\n\")\n",
    "# f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #load the preprocessed Skipgram data\n",
    "# def generate_data_skipgram_from_file():\n",
    "#     f = open('data_skipgram.txt' ,'r')\n",
    "#     for row in f:\n",
    "#         inputs,outputs = row.split(\",\")\n",
    "#         inputs = np.fromstring(inputs, dtype=int, sep=' ')\n",
    "#         inputs = np.asarray(np.split(inputs, len(inputs)))\n",
    "#         outputs = np.fromstring(outputs, dtype=float, sep=' ')\n",
    "#         outputs = np.asarray(np.split(outputs, len(inputs)))\n",
    "#         yield (inputs,outputs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create Skipgram model\n",
    "\n",
    "def getSkipgramModel(dim = 50, extra_layer = False, activation_extra_layer = 'softmax'):\n",
    "\n",
    "    # The Sequential model is a linear stack of layers.\n",
    "    skipgram = Sequential()\n",
    "    \n",
    "    # First layer (Projection layer. Embedding for wt)\n",
    "    # \n",
    "    # Parameters: \n",
    "    #   - input_dim: V (From the book: 1xV (input layer))\n",
    "    #   - embeddings_initializer = Glorot uniform. Is it for the uniform distribution of the noises?\n",
    "    #   - input_length: 1. It is needed for the dense layer (https://keras.io/layers/embeddings/#embedding)\n",
    "    # Input: One-hot ecoding of word wt in the 1xV space\n",
    "    # Output: vj in the dimension space 1xd\n",
    "    skipgram.add( Embedding(input_dim = V, output_dim = dim, embeddings_initializer = 'glorot_uniform', input_length = 1) )\n",
    "\n",
    "    # Intermediate layer. Can be seen as a helper only.\n",
    "    skipgram.add( Reshape((dim, )) )\n",
    "\n",
    "    # Second Layer (Output layer. Probabilities of context words)\n",
    "    # From documentation, Dense implements the operation: \n",
    "    #      output = activation(dot(input, kernel) + bias) \n",
    "    #   where:\n",
    "    #      activation is the element-wise activation function passed as the activation argument\n",
    "    #      kernel is a weights matrix created by the layer\n",
    "    #      and bias is a bias vector created by the layer (only applicable if use_bias is True). _WE DO NOT SET THIS_.\n",
    "    #\n",
    "    # Hence, we are computing ck*vj\n",
    "    #\n",
    "    # Parameters:\n",
    "    #   - input_dim: dim. From embedding layer, dx1\n",
    "    #   - units: V. The output dimension, From the book 1xV\n",
    "    #   - kernel_initializer: uniform (?)\n",
    "    #   - activation: softmax (as described in the book)\n",
    "    if extra_layer :\n",
    "        skipgram.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation= activation_extra_layer))\n",
    "\n",
    "    skipgram.add( Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax') )\n",
    "    \n",
    "    # Extra layer ()\n",
    "    #\n",
    "    # Justification: \n",
    "    #\n",
    "    #\n",
    "#     if extra_layer :\n",
    "#         skipgram.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation= activation_extra_layer))\n",
    "    \n",
    "    # define loss function for Skipgram\n",
    "    skipgram.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "\n",
    "    return skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Skipgram (dim = 50)...\n",
      "0 41271.35240674019\n",
      "1 39126.84181976318\n",
      "2 39288.22808551788\n",
      "3 39363.864663124084\n",
      "4 39432.38656759262\n",
      "5 39511.96853172779\n",
      "6 39608.38287115097\n",
      "7 39723.89285326004\n",
      "8 39852.42831707001\n",
      "9 39987.01795423031\n",
      "--- 178.2564082145691 seconds ---\n",
      "Running Skipgram (dim = 150)...\n",
      "0 41222.66726255417\n",
      "1 38923.405826091766\n",
      "2 38988.34154701233\n",
      "3 39005.65850484371\n",
      "4 39038.850784897804\n",
      "5 39093.706859230995\n",
      "6 39156.13746762276\n",
      "7 39216.62943792343\n",
      "8 39267.45716762543\n",
      "9 39305.05479335785\n",
      "--- 347.22299909591675 seconds ---\n",
      "Running Skipgram (dim = 300)...\n",
      "0 41159.68823719025\n",
      "1 38730.84329485893\n",
      "2 38713.535183906555\n",
      "3 38676.76882696152\n",
      "4 38648.035522818565\n",
      "5 38631.88415277004\n",
      "6 38611.77810204029\n",
      "7 38578.923778772354\n",
      "8 38533.39666950703\n",
      "9 38484.56395339966\n",
      "--- 741.4052288532257 seconds ---\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "########################## 2. Using Skipgram model #############################\n",
    "################################################################################\n",
    "\n",
    "dims = [50, 150, 300]\n",
    "iterations = 10\n",
    "listResX = []\n",
    "listResY = []\n",
    "\n",
    "for dimension in dims:\n",
    "    print(\"Running Skipgram (dim = %s)...\"% dimension)\n",
    "    skipgram = getSkipgramModel(dimension, False)\n",
    "    #print(skipgram.summary())\n",
    "    X, Y = generate_data_skipgram(corpus, window_size, V)\n",
    "\n",
    "    start_time = time.time()\n",
    "    for ite in range(iterations):\n",
    "        loss = 0.\n",
    "        for x, y in zip(X,Y):\n",
    "            loss += skipgram.train_on_batch(x, y)=\n",
    "        print(ite, loss)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(\"--- %s seconds ---\" % (end_time - start_time))\n",
    "    listResX.append(X)\n",
    "    listResY.append(Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW additional hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Skipgram (dim = 50) with activation rule 'softmax'...\n",
      "0 41623.96901512146\n",
      "1 39530.487031936646\n",
      "2 39898.54194021225\n",
      "--- 66.17264175415039 seconds ---\n",
      "Running Skipgram (dim = 50) with activation rule 'elu'...\n",
      "0 40013.837762475014\n",
      "1 38668.55372095108\n",
      "2 38269.716844677925\n",
      "--- 62.208065032958984 seconds ---\n",
      "Running Skipgram (dim = 50) with activation rule 'selu'...\n",
      "0 39887.11379611492\n",
      "1 38546.95828437805\n",
      "2 38047.002376139164\n",
      "--- 63.85457158088684 seconds ---\n",
      "Running Skipgram (dim = 50) with activation rule 'softplus'...\n",
      "0 39931.95473909378\n",
      "1 39501.52825200558\n",
      "2 39889.3167372942\n",
      "--- 62.58474898338318 seconds ---\n",
      "Running Skipgram (dim = 50) with activation rule 'softsign'...\n",
      "0 40361.39780008793\n",
      "1 38633.43089526892\n",
      "2 38564.494017481804\n",
      "--- 68.15786719322205 seconds ---\n",
      "Running Skipgram (dim = 50) with activation rule 'relu'...\n",
      "0 40091.49798643589\n",
      "1 39094.0659096241\n",
      "2 39120.28576731682\n",
      "--- 63.752196073532104 seconds ---\n",
      "Running Skipgram (dim = 50) with activation rule 'tanh'...\n",
      "0 40103.634115219116\n",
      "1 38606.355390787125\n",
      "2 38141.03712683916\n",
      "--- 64.94142389297485 seconds ---\n",
      "Running Skipgram (dim = 50) with activation rule 'sigmoid'...\n",
      "0 40013.15871787071\n",
      "1 39522.30568575859\n",
      "2 39927.890459775925\n",
      "--- 65.4682309627533 seconds ---\n",
      "Running Skipgram (dim = 50) with activation rule 'hard_sigmoid'...\n",
      "0 40037.878809571266\n",
      "1 39457.31340909004\n",
      "2 39951.18730139732\n",
      "--- 65.42859244346619 seconds ---\n",
      "Running Skipgram (dim = 50) with activation rule 'linear'...\n",
      "0 39985.72975540161\n",
      "1 38649.489277124405\n",
      "2 38200.54821574688\n",
      "--- 73.30295729637146 seconds ---\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "################### 3. Using Skipgram model (extra layer) ######################\n",
    "################################################################################\n",
    "\n",
    "#dims = [50, 150, 300]\n",
    "dims = [50]\n",
    "iterations = 3\n",
    "listResX = []\n",
    "listResY = []\n",
    "activation_rules = [\"softmax\", \"elu\", \"selu\", \"softplus\", \"softsign\", \"relu\", \"tanh\", \"sigmoid\", \"hard_sigmoid\", \"linear\"]\n",
    "for dimension in dims:\n",
    "    for activation_rule in activation_rules:\n",
    "        print(\"Running Skipgram (dim = %s) with activation rule '%s'...\"% (dimension, activation_rule))\n",
    "#         try:\n",
    "        # set extra_layer = true\n",
    "        cbow = getCbowModel(dimension, True, activation_rule)\n",
    "        X, Y = generate_data_cbow(corpus, window_size, V)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for ite in range(iterations):\n",
    "            loss = 0.\n",
    "            for x, y in zip(X,Y):\n",
    "                loss += cbow.train_on_batch(x, y)\n",
    "            print(ite, loss)\n",
    "        end_time = time.time()\n",
    "\n",
    "        print(\"--- %s seconds ---\" % (end_time - start_time))\n",
    "        listResX.append(X)\n",
    "        listResY.append(Y)\n",
    "        \n",
    "\n",
    "\n",
    "#         except:\n",
    "#             print(\"Error with\", activation_rule)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skipgram additional hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Skipgram (dim = 50) with activation rule 'softmax'...\n",
      "Error with softmax\n",
      "Running Skipgram (dim = 50) with activation rule 'elu'...\n",
      "Error with elu\n",
      "Running Skipgram (dim = 50) with activation rule 'selu'...\n",
      "Error with selu\n",
      "Running Skipgram (dim = 50) with activation rule 'softplus'...\n",
      "Error with softplus\n",
      "Running Skipgram (dim = 50) with activation rule 'softsign'...\n",
      "Error with softsign\n",
      "Running Skipgram (dim = 50) with activation rule 'relu'...\n",
      "Error with relu\n",
      "Running Skipgram (dim = 50) with activation rule 'tanh'...\n",
      "Error with tanh\n",
      "Running Skipgram (dim = 50) with activation rule 'sigmoid'...\n",
      "Error with sigmoid\n",
      "Running Skipgram (dim = 50) with activation rule 'hard_sigmoid'...\n",
      "Error with hard_sigmoid\n",
      "Running Skipgram (dim = 50) with activation rule 'linear'...\n",
      "Error with linear\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "################### 3. Using Skipgram model (extra layer) ######################\n",
    "################################################################################\n",
    "\n",
    "#dims = [50, 150, 300]\n",
    "dims = [50]\n",
    "iterations = 2\n",
    "listResX = []\n",
    "listResY = []\n",
    "activation_rules = [\"softmax\", \"elu\", \"selu\", \"softplus\", \"softsign\", \"relu\", \"tanh\", \"sigmoid\", \"hard_sigmoid\", \"linear\"]\n",
    "for dimension in dims:\n",
    "    for activation_rule in activation_rules:\n",
    "        print(\"Running Skipgram (dim = %s) with activation rule '%s'...\"% (dimension, activation_rule))\n",
    "        try:\n",
    "            # set extra_layer = true\n",
    "            skipgram = getSkipgramModel(dimension, True, activation_rule)\n",
    "            X, Y = generate_data_skipgram(corpus, window_size, V)\n",
    "\n",
    "            start_time = time.time()\n",
    "            for ite in range(iterations):\n",
    "                loss = 0.\n",
    "                for x, y in zip(X,Y):\n",
    "                    loss += skipgram.train_on_batch(x, y)\n",
    "                print(ite, loss)\n",
    "            end_time = time.time()\n",
    "\n",
    "            print(\"--- %s seconds ---\" % (end_time - start_time))\n",
    "            listResX.append(X)\n",
    "            listResY.append(Y)\n",
    "\n",
    "        except:\n",
    "            print(\"Error with\", activation_rule)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "####### 4a. Implement your own function to perform the analogy task with #######\n",
    "################################################################################\n",
    "\n",
    "embeddingWcoMatrix = \"wcoMatrix\"\n",
    "# objA is to objB as objC is a objD\n",
    "# we assume objs are strings\n",
    "def analogyFunction(objA, objB, objC, objD, embeddingType = embeddingWcoMatrix, matrix = wcoMatrix):\n",
    "    indexA = full_index[objA]\n",
    "    indexB = full_index[objB]\n",
    "    indexC = full_index[objC]\n",
    "    indexD = full_index[objD]\n",
    "    \n",
    "    if(embeddingType == embeddingWcoMatrix):\n",
    "        vecA = np.asarray(matrix[indexA,])\n",
    "        vecB = np.asarray(matrix[indexB,])\n",
    "        vecC = np.asarray(matrix[indexC,])\n",
    "        vecD = np.asarray(matrix[indexD,])\n",
    "    \n",
    "    operationResult = vecA - vecB + vecD\n",
    "    return ( np.array_equal(vecC,operationResult) ) # Not sure if this works\n",
    "\n",
    "def makeAnalogyOnFile(embeddingType = embeddingWcoMatrix, matrix = wcoMatrix):\n",
    "    # Read file\n",
    "    content = []\n",
    "    with open(\"analogy_alice.txt\") as f:\n",
    "        content = f.readlines()\n",
    "\n",
    "    # Remove whitespace characters like '\\n' at the end of each line\n",
    "    content = [x.strip() for x in content] \n",
    "    allResults = []\n",
    "    for analogy in content:\n",
    "        words = analogy.split(\" \")\n",
    "        if len(words) == 4:\n",
    "            try:\n",
    "                result = analogyFunction(words[0], words[1], words[2], words[3])\n",
    "                allResults.append(result)\n",
    "                print(\"Testing '%s': %s\" % (analogy, result))\n",
    "            except KeyError:\n",
    "                print(\"Missing words in the index \")\n",
    "    return allResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing words in the index \n",
      "Testing 'sudden suddenly usual usually': False\n",
      "Testing 'bad worse good better': False\n",
      "Testing 'go going look looking': False\n",
      "Testing 'he she his her': False\n",
      "Testing 'brother sister his her': False\n",
      "Testing 'listen listening look looking': False\n",
      "Testing 'saying said thinking thought': False\n",
      "Testing 'bird birds cat cats': False\n",
      "Testing 'good better old older': False\n",
      "Testing 'good better quick quicker': False\n",
      "Testing 'large largest good best': False\n",
      "Missing words in the index \n",
      "Testing 'falling fell knowing knew': False\n",
      "Testing 'walk walking think thinking': False\n",
      "Testing 'child children cat cats': False\n",
      "Testing 'dog dogs eye eyes': False\n",
      "Testing 'hand hands rat rats': False\n",
      "Testing 'eat eats find finds': False\n",
      "Testing 'find finds say says': False\n",
      "Testing 'old older good better': False\n",
      "Testing 'large larger quick quicker': False\n",
      "Testing 'go going listen listening': False\n",
      "Testing 'run running walk walking': False\n",
      "Testing 'run running think thinking': False\n",
      "Missing words in the index \n",
      "Testing 'say saying sit sitting': False\n",
      "Missing words in the index \n",
      "Testing 'alice she rabbit he': False\n",
      "Testing 'alice her rabbit him': False\n",
      "Testing 'alice girl rabbit sir': False\n",
      "Missing words in the index \n",
      "Missing words in the index \n",
      "Testing 'dinah cat alice girl': False\n",
      "Missing words in the index \n",
      "Testing 'his her he she': False\n",
      "Testing 'long longer quick quicker': False\n",
      "Testing 'long longer small smaller': False\n",
      "Testing 'long longer bad worse': False\n",
      "Testing 'go going look looking': False\n",
      "Testing 'listen listening look looking': False\n",
      "Testing 'swim swimming sit sitting': False\n",
      "Testing 'run running listen listening': False\n",
      "Testing 'think thinking read reading': False\n",
      "Testing 'up down close far': False\n",
      "Missing words in the index \n",
      "Missing words in the index \n",
      "Score for w-w co-ocurrence matrix embedding: 0\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "########### 4b. Compare the performance on the analogy task between ############\n",
    "########### the word embeddings you have trained in 2.1, 2.2 & 2.3  ############\n",
    "################################################################################\n",
    "\n",
    "\n",
    "# 4. Analyze the four different word embeddings\n",
    "#    - Compare the performance on the analogy task between the word embeddings that you have trained in 2.1, 2.2 and 2.3.  \n",
    "#    - Visualize your results and interpret your results\n",
    "# 5. Use the word co-occurence matrix from Question 1. Compare the performance on the analogy task with the performance of your trained word embeddings.  \n",
    "\n",
    "\n",
    "analogyResultsWcoMatrix = makeAnalogyOnFile()\n",
    "print(\"Score for w-w co-ocurrence matrix embedding: %s\" % sum(analogyResultsWcoMatrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-5cd671b3dbfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0mtsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pca'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'exact'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mplot_only\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m   \u001b[0mlow_dim_embs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mplot_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m   \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mreverse_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0mplot_with_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow_dim_embs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgettempdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tsne.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'final_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualize results\n",
    "\n",
    "# pylint: disable=missing-docstring\n",
    "# Function to draw visualization of distance between embeddings.\n",
    "def plot_with_labels(low_dim_embs, labels, filename):\n",
    "  assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "  plt.figure(figsize=(18, 18))  # in inches\n",
    "  for i, label in enumerate(labels):\n",
    "    x, y = low_dim_embs[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(label,\n",
    "                 xy=(x, y),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "\n",
    "  plt.savefig(filename)\n",
    "\n",
    "try:\n",
    "  # pylint: disable=g-import-not-at-top\n",
    "  from sklearn.manifold import TSNE\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "  tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "  plot_only = 500\n",
    "  low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "  labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "  plot_with_labels(low_dim_embs, labels, os.path.join(gettempdir(), 'tsne.png'))\n",
    "\n",
    "except ImportError as ex:\n",
    "  print('Please install sklearn, matplotlib, and scipy to show embeddings.')\n",
    "  print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "############# 5. Use the word co-occurence matrix from Question 1. #############\n",
    "################################################################################\n",
    "\n",
    "# Compare the performance on the analogy task with the performance of your trained word embeddings.  \n",
    "analogyResultsWcoMatrix = makeAnalogyOnFile()\n",
    "\n",
    "print(\"\\nScore for w-w co-ocurrence matrix embedding: %s\" % sum(analogyResultsWcoMatrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation results of the visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results of the trained word embeddings with the word-word co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Discuss**\n",
    "\n",
    "\n",
    "#### What are the main advantages of CBOW and Skipgram?\n",
    "\n",
    "\n",
    "General pros:\n",
    "It preserves the semantic relationship between words. i.e man and woman tend to be closer than man and apple.\n",
    "It uses factorization which is a well-defined problem and can be efficiently solved.\n",
    "It has to be computed once and can be used anytime once computed. \n",
    "\n",
    "CBOW:\n",
    "CBOW does not require a lot of memory. It does not need to have substantial RAM requirements like that of co-occurrence matrix where we actually need to store sometimes three huge matrices.\n",
    "This CBOW is faster to train and works well with frequent words, this lies in the fact that CBOW is learning to predict the target word based on the context. We can simply \"translate\" that as maximizing the probability of the target word by looking at the context. \n",
    "\n",
    "SKIPGRAM:\n",
    "The Skip Gram model can capture two semantics for a single word i.e it will have two vector representations of Jaguar. One for the car company and other for the animal. Skip-Gram is also more efficient with small training data and infrequent words are well presented in contrast with CBOW.\n",
    "\n",
    "To better understand the difference in lets think of an example We know that CBOW is learning to predict the word based on the context. We can simply \"translate\" that as maximizing the probability of the target word by looking at the context. This is why CBOW works better with frequent words but not the same happens for rare words. For example, given the context \"yesterday was really [...] day\" CBOW model will tell us that most probably the word is \"beautiful\" or \"nice\", which are quite usually used words. Words like \"delightful\" on the other hand will get much less attention of the model, because it is designed to predict the most probable word. This word will be smoothed over a lot of examples with more frequent words.\n",
    "\n",
    "-------------Difference-------------\n",
    "\n",
    "We know that CBOW is learning to predict the word based on the context. We can simply \"translate\" that as maximizing the probability of the target word by looking at the context. This is why CBOW works better with frequent words but not the same happens for rare words. For example, given the context \"yesterday was really [...] day\" CBOW model will tell us that most probably the word is \"beautiful\" or \"nice\", which are quite usually used words. Words like \"delightful\" on the other hand will get much less attention of the model, because it is designed to predict the most probable word. This word will be smoothed over a lot of examples with more frequent words.\n",
    "\n",
    "On the other hand, the skip-gram is designed to predict the context based on the target word. Given the word \"delightful\" it must study it and tell us, that there is huge probability, the context is \"yesterday was really [...] day\", or some other relevant context. With skip-gram the word delightful will not try to compete with word beautiful but instead, delightful+context pairs will be treated as new observations\n",
    "\n",
    "\n",
    "#### What is the advantage of negative sampling?\n",
    "\n",
    "Negative sampling tends to work better for frequent words and with low dimensional vectors. For example to optimize performance for appropriate data (frequent words,small data sets) we would use CBOW with negative sampling.\n",
    "\n",
    "Negative sampling is a way to sample the training data, similar to stochastic gradient descent, but the key is you look for negative training examples. Intuitively, it trains based on sampling places it might have expected a word, but didn't find one, which is faster than training an entire corpus every iteration and makes sense for common words.\n",
    "\n",
    "Negative sampling is developed based on noise contrastive estimation and randomly samples the words not in the context to distinguish the observed data from the artificially generated random noise.\n",
    "\n",
    "#### What are the main drawbacks of CBOW and Skipgram?\n",
    "\n",
    "-CBOW takes the average of the context of a word. For example, Apple can be both a fruit and a company but CBOW takes an average of both the contexts and places it in between a cluster for fruits and companies.\n",
    "-Training a CBOW from scratch can take forever if not properly optimized <- can really justify this\n",
    "\n",
    "Skipgram takes a substantially larger amount of time since rather than averaging the context words, each context word is used as a pairwise training example. That is, in place of one CBOW example such as [predict 'ate' from average('The', 'cat', 'the', 'mouse')], the network is presented with four skip-gram examples [predict 'ate' from 'The'], [predict 'ate' from 'cat'], [predict 'ate' from 'the'], [predict 'ate' from 'mouse']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 Load pre-trained embeddings on large corpuses (see the pdf file). You only have to consider the word embeddings with an embedding size of 300\n",
    "\n",
    "    - Compare performance on the analogy task with your own trained embeddings from \"Alice in Wonderland\". You can limit yourself to the vocabulary of Alice in Wonderland. Visualize the pre-trained word embeddings and compare these with the results of your own trained word embeddings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load pretrained word embeddings of word2vec\n",
    "path_word2vec = \"./GoogleNews-vectors-negative300.bin\"\n",
    "word2vec = KeyedVectors.load_word2vec_format(path_word2vec, binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load pretraind word embeddings of Glove\n",
    "\n",
    "#path = \"./glove.6B/glove.6B.300d_converted.txt\"\n",
    "path = \"./glove.6B/glove.6B.300d.txt\"\n",
    "\n",
    "#convert GloVe into word2vec format\n",
    "gensim.scripts.glove2word2vec.get_glove_info(path)\n",
    "gensim.scripts.glove2word2vec.glove2word2vec(path, \"glove_converted.txt\")\n",
    "\n",
    "glove = KeyedVectors.load_word2vec_format(path, binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualize the pre-trained word embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison performance with your own trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-35f6f5b763cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlistResX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "listResX[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(listResY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
